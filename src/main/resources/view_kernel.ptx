//
// Generated by NVIDIA NVVM Compiler
// Compiler built on Fri Aug  1 04:29:38 2014 (1406860178)
// Cuda compilation tools, release 6.5, V6.5.14
//

.version 4.1
.target sm_20
.address_size 64

.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd
(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
;
// sumView$__cuda_local_var_36405_29_non_const_sdata has been demoted
// sumViewTan$__cuda_local_var_36435_29_non_const_sdata has been demoted
// sumLandView$__cuda_local_var_36469_29_non_const_sdata has been demoted
// sumLandViewTan$__cuda_local_var_36503_29_non_const_sdata has been demoted
.const .align 8 .b8 __cudart_i2opi_d[144] = {8, 93, 141, 31, 177, 95, 251, 107, 234, 146, 82, 138, 247, 57, 7, 61, 123, 241, 229, 235, 199, 186, 39, 117, 45, 234, 95, 158, 102, 63, 70, 79, 183, 9, 203, 39, 207, 126, 54, 109, 31, 109, 10, 90, 139, 17, 47, 239, 15, 152, 5, 222, 255, 151, 248, 31, 59, 40, 249, 189, 139, 95, 132, 156, 244, 57, 83, 131, 57, 214, 145, 57, 65, 126, 95, 180, 38, 112, 156, 233, 132, 68, 187, 46, 245, 53, 130, 232, 62, 167, 41, 177, 28, 235, 29, 254, 28, 146, 209, 9, 234, 46, 73, 6, 224, 210, 77, 66, 58, 110, 36, 183, 97, 197, 187, 222, 171, 99, 81, 254, 65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};

.visible .entry calcRayDirect(
	.param .u32 calcRayDirect_param_0,
	.param .u32 calcRayDirect_param_1,
	.param .f32 calcRayDirect_param_2,
	.param .f32 calcRayDirect_param_3,
	.param .u64 calcRayDirect_param_4,
	.param .u32 calcRayDirect_param_5,
	.param .u32 calcRayDirect_param_6,
	.param .f32 calcRayDirect_param_7,
	.param .u32 calcRayDirect_param_8,
	.param .u64 calcRayDirect_param_9,
	.param .u32 calcRayDirect_param_10,
	.param .f32 calcRayDirect_param_11,
	.param .u64 calcRayDirect_param_12
)
{
	.reg .pred 	%p<30>;
	.reg .s16 	%rs<4>;
	.reg .s32 	%r<62>;
	.reg .f32 	%f<41>;
	.reg .s64 	%rd<19>;
	.reg .f64 	%fd<7>;


	ld.param.u32 	%r27, [calcRayDirect_param_0];
	ld.param.u32 	%r28, [calcRayDirect_param_1];
	ld.param.f32 	%f18, [calcRayDirect_param_2];
	ld.param.f32 	%f19, [calcRayDirect_param_3];
	ld.param.u64 	%rd3, [calcRayDirect_param_4];
	ld.param.u32 	%r29, [calcRayDirect_param_5];
	ld.param.u32 	%r30, [calcRayDirect_param_6];
	ld.param.f32 	%f20, [calcRayDirect_param_7];
	ld.param.u32 	%r31, [calcRayDirect_param_8];
	ld.param.u64 	%rd4, [calcRayDirect_param_9];
	ld.param.u32 	%r32, [calcRayDirect_param_10];
	ld.param.f32 	%f21, [calcRayDirect_param_11];
	ld.param.u64 	%rd5, [calcRayDirect_param_12];
	mov.u32 	%r33, %ntid.x;
	mov.u32 	%r34, %ctaid.x;
	mov.u32 	%r35, %tid.x;
	mad.lo.s32 	%r1, %r33, %r34, %r35;
	setp.ge.s32	%p3, %r1, %r29;
	@%p3 bra 	BB0_2;

	mov.u32 	%r57, 0;
	mov.u32 	%r56, %r1;
	bra.uni 	BB0_8;

BB0_2:
	add.s32 	%r2, %r30, %r29;
	setp.lt.s32	%p4, %r1, %r2;
	@%p4 bra 	BB0_7;

	shl.b32 	%r3, %r29, 1;
	add.s32 	%r4, %r3, %r30;
	setp.lt.s32	%p5, %r1, %r4;
	@%p5 bra 	BB0_6;

	shl.b32 	%r36, %r30, 1;
	add.s32 	%r37, %r36, %r3;
	setp.ge.s32	%p6, %r1, %r37;
	@%p6 bra 	BB0_24;

	sub.s32 	%r57, %r1, %r4;
	mov.u32 	%r56, 0;
	bra.uni 	BB0_8;

BB0_6:
	sub.s32 	%r56, %r1, %r2;
	add.s32 	%r57, %r30, -1;
	bra.uni 	BB0_8;

BB0_7:
	add.s32 	%r8, %r29, -1;
	sub.s32 	%r57, %r1, %r29;
	mov.u32 	%r56, %r8;

BB0_8:
	cvta.to.global.u64 	%rd6, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	mad.lo.s32 	%r61, %r29, %r28, %r27;
	mad.lo.s32 	%r13, %r57, %r29, %r56;
	cvt.s64.s32	%rd8, %r61;
	mul.wide.s32 	%rd9, %r61, 4;
	add.s64 	%rd10, %rd7, %rd9;
	ld.global.f32 	%f22, [%rd10];
	add.f32 	%f1, %f22, %f18;
	sub.s32 	%r40, %r56, %r27;
	abs.s32 	%r14, %r40;
	sub.s32 	%r41, %r57, %r28;
	abs.s32 	%r15, %r41;
	add.s64 	%rd11, %rd6, %rd8;
	mov.u16 	%rs1, 1;
	st.global.u8 	[%rd11], %rs1;
	setp.eq.s32	%p7, %r61, %r13;
	@%p7 bra 	BB0_24;

	setp.gt.s32	%p8, %r56, %r27;
	selp.b32	%r16, 1, -1, %p8;
	setp.gt.s32	%p9, %r57, %r28;
	selp.b32	%r17, 1, -1, %p9;
	sub.s32 	%r60, %r14, %r15;
	mul.f32 	%f2, %f20, %f20;
	mov.u32 	%r59, 0;
	mov.u32 	%r58, %r59;
	mov.f32 	%f38, 0fFF800000;
	mov.f32 	%f37, %f38;
	cvta.to.global.u64 	%rd15, %rd4;

BB0_10:
	neg.s32 	%r44, %r15;
	shl.b32 	%r45, %r60, 1;
	setp.gt.s32	%p10, %r45, %r44;
	selp.b32	%r46, %r16, 0, %p10;
	selp.b32	%r47, %r15, 0, %p10;
	sub.s32 	%r48, %r60, %r47;
	add.s32 	%r59, %r46, %r59;
	setp.lt.s32	%p11, %r45, %r14;
	mul.lo.s32 	%r49, %r17, %r29;
	selp.b32	%r50, %r49, 0, %p11;
	add.s32 	%r51, %r50, %r61;
	add.s32 	%r61, %r51, %r46;
	selp.b32	%r52, %r14, 0, %p11;
	add.s32 	%r60, %r48, %r52;
	selp.b32	%r53, %r17, 0, %p11;
	add.s32 	%r58, %r53, %r58;
	cvt.s64.s32	%rd1, %r61;
	mul.wide.s32 	%rd13, %r61, 4;
	add.s64 	%rd14, %rd7, %rd13;
	ld.global.f32 	%f39, [%rd14];
	abs.f32 	%f25, %f39;
	setp.gtu.f32	%p12, %f25, 0f7F800000;
	@%p12 bra 	BB0_24;

	setp.eq.s32	%p13, %r32, 0;
	mul.lo.s32 	%r54, %r58, %r58;
	mad.lo.s32 	%r55, %r59, %r59, %r54;
	cvt.rn.f32.s32	%f26, %r55;
	mul.f32 	%f6, %f2, %f26;
	@%p13 bra 	BB0_13;

	mov.f32 	%f27, 0f3F800000;
	sub.f32 	%f28, %f27, %f21;
	mul.f32 	%f29, %f28, %f6;
	div.rn.f32 	%f30, %f29, 0fCB4265A0;
	add.f32 	%f39, %f39, %f30;

BB0_13:
	setp.ne.s32	%p14, %r31, 0;
	@%p14 bra 	BB0_15;

	mov.f32 	%f40, 0f00000000;
	bra.uni 	BB0_16;

BB0_15:
	shl.b64 	%rd16, %rd1, 2;
	add.s64 	%rd17, %rd15, %rd16;
	ld.global.f32 	%f40, [%rd17];

BB0_16:
	add.f32 	%f32, %f39, %f19;
	add.f32 	%f11, %f39, %f40;
	setp.eq.f32	%p15, %f19, 0fBF800000;
	selp.f32	%f12, %f11, %f32, %p15;
	setp.le.f32	%p16, %f11, %f37;
	setp.ge.f32	%p17, %f38, 0f00000000;
	and.pred  	%p18, %p17, %p16;
	setp.le.f32	%p19, %f12, %f37;
	and.pred  	%p20, %p18, %p19;
	@%p20 bra 	BB0_23;

	sub.f32 	%f33, %f11, %f1;
	abs.f32 	%f34, %f33;
	mul.f32 	%f35, %f33, %f34;
	div.rn.f32 	%f13, %f35, %f6;
	setp.gt.f32	%p29, %f13, %f38;
	setp.ltu.f32	%p21, %f12, %f11;
	@%p21 bra 	BB0_22;

	setp.gt.f32	%p22, %f13, %f38;
	setp.eq.f32	%p23, %f12, %f11;
	and.pred  	%p24, %p23, %p22;
	add.s64 	%rd2, %rd6, %rd1;
	@%p24 bra 	BB0_21;

	sub.f32 	%f36, %f12, %f1;
	cvt.f64.f32	%fd1, %f36;
	abs.f64 	%fd2, %fd1;
	mul.f64 	%fd3, %fd1, %fd2;
	cvt.f64.f32	%fd4, %f6;
	div.rn.f64 	%fd5, %fd3, %fd4;
	cvt.f64.f32	%fd6, %f38;
	setp.leu.f64	%p25, %fd5, %fd6;
	@%p25 bra 	BB0_22;

	st.global.u8 	[%rd2], %rs1;
	bra.uni 	BB0_22;

BB0_21:
	st.global.u8 	[%rd2], %rs1;
	mov.pred 	%p29, -1;

BB0_22:
	selp.f32	%f38, %f13, %f38, %p29;
	setp.gt.f32	%p27, %f11, %f37;
	selp.f32	%f37, %f11, %f37, %p27;

BB0_23:
	setp.ne.s32	%p28, %r61, %r13;
	@%p28 bra 	BB0_10;

BB0_24:
	ret;
}

.visible .entry calcRayIndirect(
	.param .u32 calcRayIndirect_param_0,
	.param .u32 calcRayIndirect_param_1,
	.param .f32 calcRayIndirect_param_2,
	.param .f32 calcRayIndirect_param_3,
	.param .u64 calcRayIndirect_param_4,
	.param .u32 calcRayIndirect_param_5,
	.param .u32 calcRayIndirect_param_6,
	.param .f32 calcRayIndirect_param_7,
	.param .u32 calcRayIndirect_param_8,
	.param .u64 calcRayIndirect_param_9,
	.param .u32 calcRayIndirect_param_10,
	.param .f32 calcRayIndirect_param_11,
	.param .u64 calcRayIndirect_param_12
)
{
	.reg .pred 	%p<25>;
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<62>;
	.reg .f32 	%f<48>;
	.reg .s64 	%rd<22>;


	ld.param.u32 	%r27, [calcRayIndirect_param_0];
	ld.param.u32 	%r28, [calcRayIndirect_param_1];
	ld.param.f32 	%f18, [calcRayIndirect_param_2];
	ld.param.f32 	%f19, [calcRayIndirect_param_3];
	ld.param.u64 	%rd2, [calcRayIndirect_param_4];
	ld.param.u32 	%r29, [calcRayIndirect_param_5];
	ld.param.u32 	%r30, [calcRayIndirect_param_6];
	ld.param.f32 	%f20, [calcRayIndirect_param_7];
	ld.param.u32 	%r31, [calcRayIndirect_param_8];
	ld.param.u64 	%rd3, [calcRayIndirect_param_9];
	ld.param.u32 	%r32, [calcRayIndirect_param_10];
	ld.param.f32 	%f21, [calcRayIndirect_param_11];
	ld.param.u64 	%rd4, [calcRayIndirect_param_12];
	mov.u32 	%r33, %ntid.x;
	mov.u32 	%r34, %ctaid.x;
	mov.u32 	%r35, %tid.x;
	mad.lo.s32 	%r1, %r33, %r34, %r35;
	setp.ge.s32	%p1, %r1, %r29;
	@%p1 bra 	BB1_2;

	mov.u32 	%r57, 0;
	mov.u32 	%r56, %r1;
	bra.uni 	BB1_8;

BB1_2:
	add.s32 	%r2, %r30, %r29;
	setp.lt.s32	%p2, %r1, %r2;
	@%p2 bra 	BB1_7;

	shl.b32 	%r3, %r29, 1;
	add.s32 	%r4, %r3, %r30;
	setp.lt.s32	%p3, %r1, %r4;
	@%p3 bra 	BB1_6;

	shl.b32 	%r36, %r30, 1;
	add.s32 	%r37, %r36, %r3;
	setp.ge.s32	%p4, %r1, %r37;
	@%p4 bra 	BB1_25;

	sub.s32 	%r57, %r1, %r4;
	mov.u32 	%r56, 0;
	bra.uni 	BB1_8;

BB1_6:
	sub.s32 	%r56, %r1, %r2;
	add.s32 	%r57, %r30, -1;
	bra.uni 	BB1_8;

BB1_7:
	add.s32 	%r8, %r29, -1;
	sub.s32 	%r57, %r1, %r29;
	mov.u32 	%r56, %r8;

BB1_8:
	mad.lo.s32 	%r61, %r29, %r28, %r27;
	mad.lo.s32 	%r13, %r57, %r29, %r56;
	setp.ne.s32	%p5, %r31, 0;
	@%p5 bra 	BB1_10;

	mov.f32 	%f43, 0f00000000;
	bra.uni 	BB1_11;

BB1_10:
	cvta.to.global.u64 	%rd5, %rd3;
	mul.wide.s32 	%rd6, %r61, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.f32 	%f43, [%rd7];

BB1_11:
	setp.gt.f32	%p6, %f43, %f19;
	setp.neu.f32	%p7, %f19, 0fBF800000;
	and.pred  	%p8, %p7, %p6;
	@%p8 bra 	BB1_25;

	cvta.to.global.u64 	%rd8, %rd4;
	cvta.to.global.u64 	%rd9, %rd2;
	cvt.s64.s32	%rd10, %r61;
	mul.wide.s32 	%rd11, %r61, 4;
	add.s64 	%rd12, %rd9, %rd11;
	selp.f32	%f23, %f19, %f43, %p7;
	ld.global.f32 	%f24, [%rd12];
	add.f32 	%f3, %f24, %f23;
	sub.s32 	%r40, %r56, %r27;
	abs.s32 	%r14, %r40;
	sub.s32 	%r41, %r57, %r28;
	abs.s32 	%r15, %r41;
	add.s64 	%rd13, %rd8, %rd10;
	mov.u16 	%rs1, 1;
	st.global.u8 	[%rd13], %rs1;
	setp.eq.s32	%p10, %r61, %r13;
	@%p10 bra 	BB1_25;

	setp.gt.s32	%p11, %r56, %r27;
	selp.b32	%r16, 1, -1, %p11;
	setp.gt.s32	%p12, %r57, %r28;
	selp.b32	%r17, 1, -1, %p12;
	sub.s32 	%r60, %r14, %r15;
	mul.f32 	%f4, %f20, %f20;
	mov.u32 	%r59, 0;
	mov.u32 	%r58, %r59;
	mov.f32 	%f45, 0fFF800000;
	mov.f32 	%f44, %f45;

BB1_14:
	neg.s32 	%r44, %r15;
	shl.b32 	%r45, %r60, 1;
	setp.gt.s32	%p13, %r45, %r44;
	selp.b32	%r46, %r16, 0, %p13;
	selp.b32	%r47, %r15, 0, %p13;
	sub.s32 	%r48, %r60, %r47;
	add.s32 	%r59, %r46, %r59;
	setp.lt.s32	%p14, %r45, %r14;
	mul.lo.s32 	%r49, %r17, %r29;
	selp.b32	%r50, %r49, 0, %p14;
	add.s32 	%r51, %r50, %r61;
	add.s32 	%r61, %r51, %r46;
	selp.b32	%r52, %r14, 0, %p14;
	add.s32 	%r60, %r48, %r52;
	selp.b32	%r53, %r17, 0, %p14;
	add.s32 	%r58, %r53, %r58;
	cvt.s64.s32	%rd1, %r61;
	mul.wide.s32 	%rd15, %r61, 4;
	add.s64 	%rd16, %rd9, %rd15;
	ld.global.f32 	%f46, [%rd16];
	abs.f32 	%f27, %f46;
	setp.gtu.f32	%p15, %f27, 0f7F800000;
	@%p15 bra 	BB1_25;

	setp.eq.s32	%p16, %r32, 0;
	mul.lo.s32 	%r54, %r58, %r58;
	mad.lo.s32 	%r55, %r59, %r59, %r54;
	cvt.rn.f32.s32	%f28, %r55;
	mul.f32 	%f8, %f4, %f28;
	@%p16 bra 	BB1_17;

	mov.f32 	%f29, 0f3F800000;
	sub.f32 	%f30, %f29, %f21;
	mul.f32 	%f31, %f30, %f8;
	div.rn.f32 	%f32, %f31, 0fCB4265A0;
	add.f32 	%f46, %f46, %f32;

BB1_17:
	add.f32 	%f11, %f46, %f18;
	setp.gtu.f32	%p17, %f11, %f44;
	setp.ltu.f32	%p18, %f45, 0f00000000;
	or.pred  	%p19, %p18, %p17;
	@!%p19 bra 	BB1_24;
	bra.uni 	BB1_18;

BB1_18:
	sub.f32 	%f33, %f11, %f3;
	abs.f32 	%f34, %f33;
	mul.f32 	%f35, %f33, %f34;
	div.rn.f32 	%f36, %f35, %f8;
	setp.leu.f32	%p20, %f36, %f45;
	@%p20 bra 	BB1_20;

	add.s64 	%rd18, %rd8, %rd1;
	st.global.u8 	[%rd18], %rs1;

BB1_20:
	@%p5 bra 	BB1_22;

	mov.f32 	%f47, 0f00000000;
	bra.uni 	BB1_23;

BB1_22:
	cvta.to.global.u64 	%rd19, %rd3;
	shl.b64 	%rd20, %rd1, 2;
	add.s64 	%rd21, %rd19, %rd20;
	ld.global.f32 	%f47, [%rd21];

BB1_23:
	add.f32 	%f38, %f46, %f47;
	sub.f32 	%f39, %f38, %f3;
	abs.f32 	%f40, %f39;
	mul.f32 	%f41, %f39, %f40;
	div.rn.f32 	%f42, %f41, %f8;
	setp.gt.f32	%p22, %f42, %f45;
	selp.f32	%f45, %f42, %f45, %p22;
	setp.gt.f32	%p23, %f38, %f44;
	selp.f32	%f44, %f38, %f44, %p23;

BB1_24:
	setp.ne.s32	%p24, %r61, %r13;
	@%p24 bra 	BB1_14;

BB1_25:
	ret;
}

.visible .entry calcRayDirectBounded(
	.param .u32 calcRayDirectBounded_param_0,
	.param .u32 calcRayDirectBounded_param_1,
	.param .f32 calcRayDirectBounded_param_2,
	.param .f32 calcRayDirectBounded_param_3,
	.param .u64 calcRayDirectBounded_param_4,
	.param .u32 calcRayDirectBounded_param_5,
	.param .u32 calcRayDirectBounded_param_6,
	.param .f32 calcRayDirectBounded_param_7,
	.param .u32 calcRayDirectBounded_param_8,
	.param .u64 calcRayDirectBounded_param_9,
	.param .u32 calcRayDirectBounded_param_10,
	.param .f32 calcRayDirectBounded_param_11,
	.param .u64 calcRayDirectBounded_param_12,
	.param .f32 calcRayDirectBounded_param_13,
	.param .f32 calcRayDirectBounded_param_14,
	.param .f32 calcRayDirectBounded_param_15,
	.param .f32 calcRayDirectBounded_param_16,
	.param .f32 calcRayDirectBounded_param_17,
	.param .f32 calcRayDirectBounded_param_18
)
{
	.reg .pred 	%p<60>;
	.reg .s16 	%rs<4>;
	.reg .s32 	%r<74>;
	.reg .f32 	%f<46>;
	.reg .s64 	%rd<19>;
	.reg .f64 	%fd<77>;


	ld.param.u32 	%r29, [calcRayDirectBounded_param_0];
	ld.param.u32 	%r30, [calcRayDirectBounded_param_1];
	ld.param.f32 	%f18, [calcRayDirectBounded_param_2];
	ld.param.f32 	%f19, [calcRayDirectBounded_param_3];
	ld.param.u64 	%rd6, [calcRayDirectBounded_param_4];
	ld.param.u32 	%r31, [calcRayDirectBounded_param_5];
	ld.param.u32 	%r32, [calcRayDirectBounded_param_6];
	ld.param.f32 	%f20, [calcRayDirectBounded_param_7];
	ld.param.u32 	%r33, [calcRayDirectBounded_param_8];
	ld.param.u64 	%rd7, [calcRayDirectBounded_param_9];
	ld.param.u32 	%r34, [calcRayDirectBounded_param_10];
	ld.param.f32 	%f21, [calcRayDirectBounded_param_11];
	ld.param.u64 	%rd8, [calcRayDirectBounded_param_12];
	ld.param.f32 	%f22, [calcRayDirectBounded_param_13];
	ld.param.f32 	%f23, [calcRayDirectBounded_param_14];
	ld.param.f32 	%f24, [calcRayDirectBounded_param_15];
	ld.param.f32 	%f25, [calcRayDirectBounded_param_16];
	ld.param.f32 	%f43, [calcRayDirectBounded_param_17];
	ld.param.f32 	%f27, [calcRayDirectBounded_param_18];
	mov.u32 	%r35, %ntid.x;
	mov.u32 	%r36, %ctaid.x;
	mov.u32 	%r37, %tid.x;
	mad.lo.s32 	%r1, %r35, %r36, %r37;
	setp.ge.s32	%p5, %r1, %r31;
	@%p5 bra 	BB2_2;

	mov.u32 	%r69, 0;
	mov.u32 	%r68, %r1;
	bra.uni 	BB2_8;

BB2_2:
	add.s32 	%r2, %r32, %r31;
	setp.lt.s32	%p6, %r1, %r2;
	@%p6 bra 	BB2_7;

	shl.b32 	%r3, %r31, 1;
	add.s32 	%r4, %r3, %r32;
	setp.lt.s32	%p7, %r1, %r4;
	@%p7 bra 	BB2_6;

	shl.b32 	%r38, %r32, 1;
	add.s32 	%r39, %r38, %r3;
	setp.ge.s32	%p8, %r1, %r39;
	@%p8 bra 	BB2_36;

	sub.s32 	%r69, %r1, %r4;
	mov.u32 	%r68, 0;
	bra.uni 	BB2_8;

BB2_6:
	sub.s32 	%r68, %r1, %r2;
	add.s32 	%r69, %r32, -1;
	bra.uni 	BB2_8;

BB2_7:
	add.s32 	%r8, %r31, -1;
	sub.s32 	%r69, %r1, %r31;
	mov.u32 	%r68, %r8;

BB2_8:
	cvta.to.global.u64 	%rd1, %rd8;
	cvta.to.global.u64 	%rd2, %rd6;
	cvt.rn.f64.s32	%fd10, %r69;
	cvt.rn.f64.s32	%fd11, %r30;
	sub.f64 	%fd12, %fd11, %fd10;
	cvt.rn.f64.s32	%fd13, %r29;
	cvt.rn.f64.s32	%fd14, %r68;
	sub.f64 	%fd15, %fd14, %fd13;
	abs.f64 	%fd1, %fd15;
	abs.f64 	%fd2, %fd12;
	setp.eq.f64	%p9, %fd1, 0d0000000000000000;
	setp.eq.f64	%p10, %fd2, 0d0000000000000000;
	and.pred  	%p11, %p9, %p10;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r12}, %fd15;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r42}, %fd12;
	}
	and.b32  	%r13, %r42, -2147483648;
	@%p11 bra 	BB2_12;

	setp.eq.f64	%p12, %fd1, 0d7FF0000000000000;
	setp.eq.f64	%p13, %fd2, 0d7FF0000000000000;
	and.pred  	%p14, %p12, %p13;
	@%p14 bra 	BB2_11;

	setp.lt.s32	%p15, %r12, 0;
	min.f64 	%fd16, %fd2, %fd1;
	max.f64 	%fd17, %fd2, %fd1;
	div.rn.f64 	%fd18, %fd16, %fd17;
	mul.f64 	%fd19, %fd18, %fd18;
	mov.f64 	%fd20, 0d3F2D3B63DBB65B49;
	mov.f64 	%fd21, 0dBEF53E1D2A25FF7E;
	fma.rn.f64 	%fd22, %fd21, %fd19, %fd20;
	mov.f64 	%fd23, 0dBF5312788DDE082E;
	fma.rn.f64 	%fd24, %fd22, %fd19, %fd23;
	mov.f64 	%fd25, 0d3F6F9690C8249315;
	fma.rn.f64 	%fd26, %fd24, %fd19, %fd25;
	mov.f64 	%fd27, 0dBF82CF5AABC7CF0D;
	fma.rn.f64 	%fd28, %fd26, %fd19, %fd27;
	mov.f64 	%fd29, 0d3F9162B0B2A3BFDE;
	fma.rn.f64 	%fd30, %fd28, %fd19, %fd29;
	mov.f64 	%fd31, 0dBF9A7256FEB6FC6B;
	fma.rn.f64 	%fd32, %fd30, %fd19, %fd31;
	mov.f64 	%fd33, 0d3FA171560CE4A489;
	fma.rn.f64 	%fd34, %fd32, %fd19, %fd33;
	mov.f64 	%fd35, 0dBFA4F44D841450E4;
	fma.rn.f64 	%fd36, %fd34, %fd19, %fd35;
	mov.f64 	%fd37, 0d3FA7EE3D3F36BB95;
	fma.rn.f64 	%fd38, %fd36, %fd19, %fd37;
	mov.f64 	%fd39, 0dBFAAD32AE04A9FD1;
	fma.rn.f64 	%fd40, %fd38, %fd19, %fd39;
	mov.f64 	%fd41, 0d3FAE17813D66954F;
	fma.rn.f64 	%fd42, %fd40, %fd19, %fd41;
	mov.f64 	%fd43, 0dBFB11089CA9A5BCD;
	fma.rn.f64 	%fd44, %fd42, %fd19, %fd43;
	mov.f64 	%fd45, 0d3FB3B12B2DB51738;
	fma.rn.f64 	%fd46, %fd44, %fd19, %fd45;
	mov.f64 	%fd47, 0dBFB745D022F8DC5C;
	fma.rn.f64 	%fd48, %fd46, %fd19, %fd47;
	mov.f64 	%fd49, 0d3FBC71C709DFE927;
	fma.rn.f64 	%fd50, %fd48, %fd19, %fd49;
	mov.f64 	%fd51, 0dBFC2492491FA1744;
	fma.rn.f64 	%fd52, %fd50, %fd19, %fd51;
	mov.f64 	%fd53, 0d3FC99999999840D2;
	fma.rn.f64 	%fd54, %fd52, %fd19, %fd53;
	mov.f64 	%fd55, 0dBFD555555555544C;
	fma.rn.f64 	%fd56, %fd54, %fd19, %fd55;
	mul.f64 	%fd57, %fd56, %fd19;
	fma.rn.f64 	%fd58, %fd57, %fd18, %fd18;
	mov.f64 	%fd59, 0d3FF921FB54442D18;
	sub.f64 	%fd60, %fd59, %fd58;
	setp.gt.f64	%p16, %fd2, %fd1;
	selp.f64	%fd61, %fd60, %fd58, %p16;
	mov.f64 	%fd62, 0d400921FB54442D18;
	sub.f64 	%fd63, %fd62, %fd61;
	selp.f64	%fd64, %fd63, %fd61, %p15;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r43, %temp}, %fd64;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r44}, %fd64;
	}
	or.b32  	%r45, %r44, %r13;
	mov.b64 	%fd65, {%r43, %r45};
	add.f64 	%fd66, %fd1, %fd2;
	setp.gtu.f64	%p17, %fd66, 0d7FF0000000000000;
	selp.f64	%fd76, %fd66, %fd65, %p17;
	bra.uni 	BB2_13;

BB2_11:
	setp.lt.s32	%p18, %r12, 0;
	selp.f64	%fd67, 0d4002D97C7F3321D2, 0d3FE921FB54442D18, %p18;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r46, %temp}, %fd67;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r47}, %fd67;
	}
	or.b32  	%r48, %r47, %r13;
	mov.b64 	%fd76, {%r46, %r48};
	bra.uni 	BB2_13;

BB2_12:
	setp.lt.s32	%p19, %r12, 0;
	selp.f64	%fd68, 0d400921FB54442D18, 0d0000000000000000, %p19;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r49, %temp}, %fd68;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r50}, %fd68;
	}
	or.b32  	%r51, %r50, %r13;
	mov.b64 	%fd76, {%r49, %r51};

BB2_13:
	add.f64 	%fd69, %fd76, 0d401921FB54442D18;
	setp.lt.f64	%p20, %fd76, 0d0000000000000000;
	selp.f64	%fd7, %fd69, %fd76, %p20;
	cvt.f64.f32	%fd8, %f25;
	setp.ge.f64	%p21, %fd7, %fd8;
	setp.lt.f32	%p22, %f25, %f24;
	and.pred  	%p23, %p22, %p21;
	cvt.f64.f32	%fd9, %f24;
	setp.le.f64	%p24, %fd7, %fd9;
	and.pred  	%p25, %p23, %p24;
	@%p25 bra 	BB2_16;

	setp.ltu.f32	%p26, %f25, %f24;
	@%p26 bra 	BB2_36;

	setp.ltu.f64	%p27, %fd7, %fd8;
	setp.gtu.f64	%p28, %fd7, %fd9;
	and.pred  	%p29, %p27, %p28;
	@%p29 bra 	BB2_36;

BB2_16:
	mad.lo.s32 	%r73, %r31, %r30, %r29;
	mad.lo.s32 	%r15, %r69, %r31, %r68;
	cvt.s64.s32	%rd3, %r73;
	mul.wide.s32 	%rd9, %r73, 4;
	add.s64 	%rd10, %rd2, %rd9;
	ld.global.f32 	%f28, [%rd10];
	add.f32 	%f1, %f28, %f18;
	sub.s32 	%r52, %r68, %r29;
	abs.s32 	%r16, %r52;
	sub.s32 	%r53, %r69, %r30;
	abs.s32 	%r17, %r53;
	setp.eq.f32	%p30, %f22, 0f00000000;
	setp.eq.f32	%p31, %f43, 0fFF800000;
	and.pred  	%p32, %p31, %p30;
	@!%p32 bra 	BB2_18;
	bra.uni 	BB2_17;

BB2_17:
	add.s64 	%rd11, %rd1, %rd3;
	mov.u16 	%rs1, 1;
	st.global.u8 	[%rd11], %rs1;

BB2_18:
	setp.eq.s32	%p33, %r73, %r15;
	@%p33 bra 	BB2_36;

	setp.gt.s32	%p34, %r68, %r29;
	selp.b32	%r18, 1, -1, %p34;
	setp.gt.s32	%p35, %r69, %r30;
	selp.b32	%r19, 1, -1, %p35;
	sub.s32 	%r72, %r16, %r17;
	mul.f32 	%f2, %f20, %f20;
	mov.u32 	%r71, 0;
	mov.u32 	%r70, %r71;
	mov.f32 	%f42, 0fFF800000;
	cvta.to.global.u64 	%rd15, %rd7;

BB2_20:
	neg.s32 	%r56, %r17;
	shl.b32 	%r57, %r72, 1;
	setp.gt.s32	%p36, %r57, %r56;
	selp.b32	%r58, %r18, 0, %p36;
	selp.b32	%r59, %r17, 0, %p36;
	sub.s32 	%r60, %r72, %r59;
	add.s32 	%r71, %r58, %r71;
	setp.lt.s32	%p37, %r57, %r16;
	mul.lo.s32 	%r61, %r19, %r31;
	selp.b32	%r62, %r61, 0, %p37;
	add.s32 	%r63, %r62, %r73;
	add.s32 	%r73, %r63, %r58;
	selp.b32	%r64, %r16, 0, %p37;
	add.s32 	%r72, %r60, %r64;
	selp.b32	%r65, %r19, 0, %p37;
	add.s32 	%r70, %r65, %r70;
	cvt.s64.s32	%rd4, %r73;
	mul.wide.s32 	%rd13, %r73, 4;
	add.s64 	%rd14, %rd2, %rd13;
	ld.global.f32 	%f44, [%rd14];
	abs.f32 	%f30, %f44;
	setp.gtu.f32	%p38, %f30, 0f7F800000;
	@%p38 bra 	BB2_36;

	mul.lo.s32 	%r66, %r70, %r70;
	mad.lo.s32 	%r67, %r71, %r71, %r66;
	cvt.rn.f32.s32	%f31, %r67;
	mul.f32 	%f6, %f2, %f31;
	setp.ge.f32	%p39, %f6, %f23;
	@%p39 bra 	BB2_36;

	setp.eq.s32	%p40, %r34, 0;
	@%p40 bra 	BB2_24;

	mov.f32 	%f32, 0f3F800000;
	sub.f32 	%f33, %f32, %f21;
	mul.f32 	%f34, %f33, %f6;
	div.rn.f32 	%f35, %f34, 0fCB4265A0;
	add.f32 	%f44, %f44, %f35;

BB2_24:
	setp.ne.s32	%p41, %r33, 0;
	@%p41 bra 	BB2_26;

	mov.f32 	%f45, 0f00000000;
	bra.uni 	BB2_27;

BB2_26:
	shl.b64 	%rd16, %rd4, 2;
	add.s64 	%rd17, %rd15, %rd16;
	ld.global.f32 	%f45, [%rd17];

BB2_27:
	add.f32 	%f37, %f44, %f19;
	add.f32 	%f11, %f44, %f45;
	setp.eq.f32	%p42, %f19, 0fBF800000;
	selp.f32	%f12, %f11, %f37, %p42;
	setp.le.f32	%p43, %f11, %f42;
	setp.ge.f32	%p44, %f43, 0f00000000;
	and.pred  	%p45, %p44, %p43;
	setp.le.f32	%p46, %f12, %f42;
	and.pred  	%p47, %p45, %p46;
	@%p47 bra 	BB2_35;

	sub.f32 	%f38, %f11, %f1;
	abs.f32 	%f39, %f38;
	mul.f32 	%f40, %f38, %f39;
	div.rn.f32 	%f13, %f40, %f6;
	setp.gt.f32	%p48, %f13, %f27;
	@%p48 bra 	BB2_36;

	setp.ge.f32	%p49, %f12, %f11;
	setp.ge.f32	%p50, %f6, %f22;
	and.pred  	%p51, %p50, %p49;
	setp.gt.f32	%p59, %f13, %f43;
	@!%p51 bra 	BB2_34;
	bra.uni 	BB2_30;

BB2_30:
	setp.gt.f32	%p59, %f13, %f43;
	setp.eq.f32	%p53, %f12, %f11;
	and.pred  	%p54, %p53, %p59;
	add.s64 	%rd5, %rd1, %rd4;
	@%p54 bra 	BB2_33;

	sub.f32 	%f41, %f12, %f1;
	cvt.f64.f32	%fd70, %f41;
	abs.f64 	%fd71, %fd70;
	mul.f64 	%fd72, %fd70, %fd71;
	cvt.f64.f32	%fd73, %f6;
	div.rn.f64 	%fd74, %fd72, %fd73;
	cvt.f64.f32	%fd75, %f43;
	setp.leu.f64	%p55, %fd74, %fd75;
	@%p55 bra 	BB2_34;

	mov.u16 	%rs2, 1;
	st.global.u8 	[%rd5], %rs2;
	setp.gt.f32	%p59, %f13, %f43;
	bra.uni 	BB2_34;

BB2_33:
	mov.u16 	%rs3, 1;
	st.global.u8 	[%rd5], %rs3;
	mov.pred 	%p59, -1;

BB2_34:
	selp.f32	%f43, %f13, %f43, %p59;
	setp.gt.f32	%p57, %f11, %f42;
	selp.f32	%f42, %f11, %f42, %p57;

BB2_35:
	setp.ne.s32	%p58, %r73, %r15;
	@%p58 bra 	BB2_20;

BB2_36:
	ret;
}

.visible .entry calcRayIndirectBounded(
	.param .u32 calcRayIndirectBounded_param_0,
	.param .u32 calcRayIndirectBounded_param_1,
	.param .f32 calcRayIndirectBounded_param_2,
	.param .f32 calcRayIndirectBounded_param_3,
	.param .u64 calcRayIndirectBounded_param_4,
	.param .u32 calcRayIndirectBounded_param_5,
	.param .u32 calcRayIndirectBounded_param_6,
	.param .f32 calcRayIndirectBounded_param_7,
	.param .u32 calcRayIndirectBounded_param_8,
	.param .u64 calcRayIndirectBounded_param_9,
	.param .u32 calcRayIndirectBounded_param_10,
	.param .f32 calcRayIndirectBounded_param_11,
	.param .u64 calcRayIndirectBounded_param_12,
	.param .f32 calcRayIndirectBounded_param_13,
	.param .f32 calcRayIndirectBounded_param_14,
	.param .f32 calcRayIndirectBounded_param_15,
	.param .f32 calcRayIndirectBounded_param_16,
	.param .f32 calcRayIndirectBounded_param_17,
	.param .f32 calcRayIndirectBounded_param_18
)
{
	.reg .pred 	%p<54>;
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<74>;
	.reg .f32 	%f<53>;
	.reg .s64 	%rd<22>;
	.reg .f64 	%fd<71>;


	ld.param.u32 	%r29, [calcRayIndirectBounded_param_0];
	ld.param.u32 	%r30, [calcRayIndirectBounded_param_1];
	ld.param.f32 	%f20, [calcRayIndirectBounded_param_2];
	ld.param.f32 	%f21, [calcRayIndirectBounded_param_3];
	ld.param.u64 	%rd6, [calcRayIndirectBounded_param_4];
	ld.param.u32 	%r31, [calcRayIndirectBounded_param_5];
	ld.param.u32 	%r32, [calcRayIndirectBounded_param_6];
	ld.param.f32 	%f22, [calcRayIndirectBounded_param_7];
	ld.param.u32 	%r33, [calcRayIndirectBounded_param_8];
	ld.param.u64 	%rd7, [calcRayIndirectBounded_param_9];
	ld.param.u32 	%r34, [calcRayIndirectBounded_param_10];
	ld.param.f32 	%f23, [calcRayIndirectBounded_param_11];
	ld.param.u64 	%rd8, [calcRayIndirectBounded_param_12];
	ld.param.f32 	%f24, [calcRayIndirectBounded_param_13];
	ld.param.f32 	%f25, [calcRayIndirectBounded_param_14];
	ld.param.f32 	%f26, [calcRayIndirectBounded_param_15];
	ld.param.f32 	%f27, [calcRayIndirectBounded_param_16];
	ld.param.f32 	%f50, [calcRayIndirectBounded_param_17];
	ld.param.f32 	%f29, [calcRayIndirectBounded_param_18];
	mov.u32 	%r35, %ntid.x;
	mov.u32 	%r36, %ctaid.x;
	mov.u32 	%r37, %tid.x;
	mad.lo.s32 	%r1, %r35, %r36, %r37;
	setp.ge.s32	%p1, %r1, %r31;
	@%p1 bra 	BB3_2;

	mov.u32 	%r69, 0;
	mov.u32 	%r68, %r1;
	bra.uni 	BB3_8;

BB3_2:
	add.s32 	%r2, %r32, %r31;
	setp.lt.s32	%p2, %r1, %r2;
	@%p2 bra 	BB3_7;

	shl.b32 	%r3, %r31, 1;
	add.s32 	%r4, %r3, %r32;
	setp.lt.s32	%p3, %r1, %r4;
	@%p3 bra 	BB3_6;

	shl.b32 	%r38, %r32, 1;
	add.s32 	%r39, %r38, %r3;
	setp.ge.s32	%p4, %r1, %r39;
	@%p4 bra 	BB3_38;

	sub.s32 	%r69, %r1, %r4;
	mov.u32 	%r68, 0;
	bra.uni 	BB3_8;

BB3_6:
	sub.s32 	%r68, %r1, %r2;
	add.s32 	%r69, %r32, -1;
	bra.uni 	BB3_8;

BB3_7:
	add.s32 	%r8, %r31, -1;
	sub.s32 	%r69, %r1, %r31;
	mov.u32 	%r68, %r8;

BB3_8:
	cvta.to.global.u64 	%rd1, %rd8;
	cvta.to.global.u64 	%rd2, %rd6;
	cvta.to.global.u64 	%rd3, %rd7;
	cvt.rn.f64.s32	%fd10, %r69;
	cvt.rn.f64.s32	%fd11, %r30;
	sub.f64 	%fd12, %fd11, %fd10;
	cvt.rn.f64.s32	%fd13, %r29;
	cvt.rn.f64.s32	%fd14, %r68;
	sub.f64 	%fd15, %fd14, %fd13;
	abs.f64 	%fd1, %fd15;
	abs.f64 	%fd2, %fd12;
	setp.eq.f64	%p5, %fd1, 0d0000000000000000;
	setp.eq.f64	%p6, %fd2, 0d0000000000000000;
	and.pred  	%p7, %p5, %p6;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r12}, %fd15;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r42}, %fd12;
	}
	and.b32  	%r13, %r42, -2147483648;
	@%p7 bra 	BB3_12;

	setp.eq.f64	%p8, %fd1, 0d7FF0000000000000;
	setp.eq.f64	%p9, %fd2, 0d7FF0000000000000;
	and.pred  	%p10, %p8, %p9;
	@%p10 bra 	BB3_11;

	setp.lt.s32	%p11, %r12, 0;
	min.f64 	%fd16, %fd2, %fd1;
	max.f64 	%fd17, %fd2, %fd1;
	div.rn.f64 	%fd18, %fd16, %fd17;
	mul.f64 	%fd19, %fd18, %fd18;
	mov.f64 	%fd20, 0d3F2D3B63DBB65B49;
	mov.f64 	%fd21, 0dBEF53E1D2A25FF7E;
	fma.rn.f64 	%fd22, %fd21, %fd19, %fd20;
	mov.f64 	%fd23, 0dBF5312788DDE082E;
	fma.rn.f64 	%fd24, %fd22, %fd19, %fd23;
	mov.f64 	%fd25, 0d3F6F9690C8249315;
	fma.rn.f64 	%fd26, %fd24, %fd19, %fd25;
	mov.f64 	%fd27, 0dBF82CF5AABC7CF0D;
	fma.rn.f64 	%fd28, %fd26, %fd19, %fd27;
	mov.f64 	%fd29, 0d3F9162B0B2A3BFDE;
	fma.rn.f64 	%fd30, %fd28, %fd19, %fd29;
	mov.f64 	%fd31, 0dBF9A7256FEB6FC6B;
	fma.rn.f64 	%fd32, %fd30, %fd19, %fd31;
	mov.f64 	%fd33, 0d3FA171560CE4A489;
	fma.rn.f64 	%fd34, %fd32, %fd19, %fd33;
	mov.f64 	%fd35, 0dBFA4F44D841450E4;
	fma.rn.f64 	%fd36, %fd34, %fd19, %fd35;
	mov.f64 	%fd37, 0d3FA7EE3D3F36BB95;
	fma.rn.f64 	%fd38, %fd36, %fd19, %fd37;
	mov.f64 	%fd39, 0dBFAAD32AE04A9FD1;
	fma.rn.f64 	%fd40, %fd38, %fd19, %fd39;
	mov.f64 	%fd41, 0d3FAE17813D66954F;
	fma.rn.f64 	%fd42, %fd40, %fd19, %fd41;
	mov.f64 	%fd43, 0dBFB11089CA9A5BCD;
	fma.rn.f64 	%fd44, %fd42, %fd19, %fd43;
	mov.f64 	%fd45, 0d3FB3B12B2DB51738;
	fma.rn.f64 	%fd46, %fd44, %fd19, %fd45;
	mov.f64 	%fd47, 0dBFB745D022F8DC5C;
	fma.rn.f64 	%fd48, %fd46, %fd19, %fd47;
	mov.f64 	%fd49, 0d3FBC71C709DFE927;
	fma.rn.f64 	%fd50, %fd48, %fd19, %fd49;
	mov.f64 	%fd51, 0dBFC2492491FA1744;
	fma.rn.f64 	%fd52, %fd50, %fd19, %fd51;
	mov.f64 	%fd53, 0d3FC99999999840D2;
	fma.rn.f64 	%fd54, %fd52, %fd19, %fd53;
	mov.f64 	%fd55, 0dBFD555555555544C;
	fma.rn.f64 	%fd56, %fd54, %fd19, %fd55;
	mul.f64 	%fd57, %fd56, %fd19;
	fma.rn.f64 	%fd58, %fd57, %fd18, %fd18;
	mov.f64 	%fd59, 0d3FF921FB54442D18;
	sub.f64 	%fd60, %fd59, %fd58;
	setp.gt.f64	%p12, %fd2, %fd1;
	selp.f64	%fd61, %fd60, %fd58, %p12;
	mov.f64 	%fd62, 0d400921FB54442D18;
	sub.f64 	%fd63, %fd62, %fd61;
	selp.f64	%fd64, %fd63, %fd61, %p11;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r43, %temp}, %fd64;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r44}, %fd64;
	}
	or.b32  	%r45, %r44, %r13;
	mov.b64 	%fd65, {%r43, %r45};
	add.f64 	%fd66, %fd1, %fd2;
	setp.gtu.f64	%p13, %fd66, 0d7FF0000000000000;
	selp.f64	%fd70, %fd66, %fd65, %p13;
	bra.uni 	BB3_13;

BB3_11:
	setp.lt.s32	%p14, %r12, 0;
	selp.f64	%fd67, 0d4002D97C7F3321D2, 0d3FE921FB54442D18, %p14;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r46, %temp}, %fd67;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r47}, %fd67;
	}
	or.b32  	%r48, %r47, %r13;
	mov.b64 	%fd70, {%r46, %r48};
	bra.uni 	BB3_13;

BB3_12:
	setp.lt.s32	%p15, %r12, 0;
	selp.f64	%fd68, 0d400921FB54442D18, 0d0000000000000000, %p15;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r49, %temp}, %fd68;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r50}, %fd68;
	}
	or.b32  	%r51, %r50, %r13;
	mov.b64 	%fd70, {%r49, %r51};

BB3_13:
	add.f64 	%fd69, %fd70, 0d401921FB54442D18;
	setp.lt.f64	%p16, %fd70, 0d0000000000000000;
	selp.f64	%fd7, %fd69, %fd70, %p16;
	cvt.f64.f32	%fd8, %f27;
	setp.ge.f64	%p17, %fd7, %fd8;
	setp.lt.f32	%p18, %f27, %f26;
	and.pred  	%p19, %p18, %p17;
	cvt.f64.f32	%fd9, %f26;
	setp.le.f64	%p20, %fd7, %fd9;
	and.pred  	%p21, %p19, %p20;
	@%p21 bra 	BB3_16;

	setp.ltu.f32	%p22, %f27, %f26;
	@%p22 bra 	BB3_38;

	setp.ltu.f64	%p23, %fd7, %fd8;
	setp.gtu.f64	%p24, %fd7, %fd9;
	and.pred  	%p25, %p23, %p24;
	@%p25 bra 	BB3_38;

BB3_16:
	mad.lo.s32 	%r73, %r31, %r30, %r29;
	mad.lo.s32 	%r15, %r69, %r31, %r68;
	setp.ne.s32	%p26, %r33, 0;
	@%p26 bra 	BB3_18;

	mov.f32 	%f48, 0f00000000;
	bra.uni 	BB3_19;

BB3_18:
	mul.wide.s32 	%rd9, %r73, 4;
	add.s64 	%rd10, %rd3, %rd9;
	ld.global.f32 	%f48, [%rd10];

BB3_19:
	setp.gt.f32	%p27, %f48, %f21;
	setp.neu.f32	%p28, %f21, 0fBF800000;
	and.pred  	%p29, %p28, %p27;
	@%p29 bra 	BB3_38;

	cvt.s64.s32	%rd4, %r73;
	mul.wide.s32 	%rd11, %r73, 4;
	add.s64 	%rd12, %rd2, %rd11;
	selp.f32	%f31, %f21, %f48, %p28;
	ld.global.f32 	%f32, [%rd12];
	add.f32 	%f3, %f32, %f31;
	sub.s32 	%r52, %r68, %r29;
	abs.s32 	%r16, %r52;
	sub.s32 	%r53, %r69, %r30;
	abs.s32 	%r17, %r53;
	setp.eq.f32	%p31, %f24, 0f00000000;
	setp.eq.f32	%p32, %f50, 0fFF800000;
	and.pred  	%p33, %p32, %p31;
	@!%p33 bra 	BB3_22;
	bra.uni 	BB3_21;

BB3_21:
	add.s64 	%rd13, %rd1, %rd4;
	mov.u16 	%rs1, 1;
	st.global.u8 	[%rd13], %rs1;

BB3_22:
	setp.eq.s32	%p34, %r73, %r15;
	@%p34 bra 	BB3_38;

	setp.gt.s32	%p35, %r68, %r29;
	selp.b32	%r18, 1, -1, %p35;
	setp.gt.s32	%p36, %r69, %r30;
	selp.b32	%r19, 1, -1, %p36;
	sub.s32 	%r72, %r16, %r17;
	mul.f32 	%f4, %f22, %f22;
	mov.u32 	%r71, 0;
	mov.u32 	%r70, %r71;
	mov.f32 	%f49, 0fFF800000;

BB3_24:
	neg.s32 	%r56, %r17;
	shl.b32 	%r57, %r72, 1;
	setp.gt.s32	%p37, %r57, %r56;
	selp.b32	%r58, %r18, 0, %p37;
	selp.b32	%r59, %r17, 0, %p37;
	sub.s32 	%r60, %r72, %r59;
	add.s32 	%r71, %r58, %r71;
	setp.lt.s32	%p38, %r57, %r16;
	mul.lo.s32 	%r61, %r19, %r31;
	selp.b32	%r62, %r61, 0, %p38;
	add.s32 	%r63, %r62, %r73;
	add.s32 	%r73, %r63, %r58;
	selp.b32	%r64, %r16, 0, %p38;
	add.s32 	%r72, %r60, %r64;
	selp.b32	%r65, %r19, 0, %p38;
	add.s32 	%r70, %r65, %r70;
	cvt.s64.s32	%rd5, %r73;
	mul.wide.s32 	%rd15, %r73, 4;
	add.s64 	%rd16, %rd2, %rd15;
	ld.global.f32 	%f51, [%rd16];
	abs.f32 	%f34, %f51;
	setp.gtu.f32	%p39, %f34, 0f7F800000;
	@%p39 bra 	BB3_38;

	mul.lo.s32 	%r66, %r70, %r70;
	mad.lo.s32 	%r67, %r71, %r71, %r66;
	cvt.rn.f32.s32	%f35, %r67;
	mul.f32 	%f8, %f4, %f35;
	setp.ge.f32	%p40, %f8, %f25;
	@%p40 bra 	BB3_38;

	setp.eq.s32	%p41, %r34, 0;
	@%p41 bra 	BB3_28;

	mov.f32 	%f36, 0f3F800000;
	sub.f32 	%f37, %f36, %f23;
	mul.f32 	%f38, %f37, %f8;
	div.rn.f32 	%f39, %f38, 0fCB4265A0;
	add.f32 	%f51, %f51, %f39;

BB3_28:
	add.f32 	%f11, %f51, %f20;
	setp.gtu.f32	%p42, %f11, %f49;
	setp.ltu.f32	%p43, %f50, 0f00000000;
	or.pred  	%p44, %p43, %p42;
	@!%p44 bra 	BB3_37;
	bra.uni 	BB3_29;

BB3_29:
	sub.f32 	%f40, %f11, %f3;
	abs.f32 	%f41, %f40;
	mul.f32 	%f42, %f40, %f41;
	div.rn.f32 	%f12, %f42, %f8;
	setp.leu.f32	%p45, %f12, %f50;
	@%p45 bra 	BB3_32;

	setp.le.f32	%p46, %f12, %f29;
	setp.ge.f32	%p47, %f8, %f24;
	and.pred  	%p48, %p47, %p46;
	@!%p48 bra 	BB3_32;
	bra.uni 	BB3_31;

BB3_31:
	add.s64 	%rd18, %rd1, %rd5;
	mov.u16 	%rs2, 1;
	st.global.u8 	[%rd18], %rs2;

BB3_32:
	@%p26 bra 	BB3_34;

	mov.f32 	%f52, 0f00000000;
	bra.uni 	BB3_35;

BB3_34:
	shl.b64 	%rd20, %rd5, 2;
	add.s64 	%rd21, %rd3, %rd20;
	ld.global.f32 	%f52, [%rd21];

BB3_35:
	add.f32 	%f15, %f51, %f52;
	sub.f32 	%f44, %f15, %f3;
	abs.f32 	%f45, %f44;
	mul.f32 	%f46, %f44, %f45;
	div.rn.f32 	%f47, %f46, %f8;
	setp.gt.f32	%p50, %f47, %f50;
	selp.f32	%f50, %f47, %f50, %p50;
	setp.gt.f32	%p51, %f50, %f29;
	@%p51 bra 	BB3_38;

	setp.gt.f32	%p52, %f15, %f49;
	selp.f32	%f49, %f15, %f49, %p52;

BB3_37:
	setp.ne.s32	%p53, %r73, %r15;
	@%p53 bra 	BB3_24;

BB3_38:
	ret;
}

.visible .entry calcRayTan(
	.param .u32 calcRayTan_param_0,
	.param .u32 calcRayTan_param_1,
	.param .f64 calcRayTan_param_2,
	.param .u64 calcRayTan_param_3,
	.param .u32 calcRayTan_param_4,
	.param .u32 calcRayTan_param_5,
	.param .f64 calcRayTan_param_6,
	.param .u32 calcRayTan_param_7,
	.param .u64 calcRayTan_param_8,
	.param .u32 calcRayTan_param_9,
	.param .f32 calcRayTan_param_10,
	.param .u64 calcRayTan_param_11,
	.param .u32 calcRayTan_param_12,
	.param .f64 calcRayTan_param_13,
	.param .f64 calcRayTan_param_14,
	.param .f64 calcRayTan_param_15,
	.param .f64 calcRayTan_param_16,
	.param .f64 calcRayTan_param_17,
	.param .f64 calcRayTan_param_18,
	.param .f64 calcRayTan_param_19
)
{
	.local .align 4 .b8 	__local_depot4[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<79>;
	.reg .s32 	%r<149>;
	.reg .f32 	%f<11>;
	.reg .s64 	%rd<31>;
	.reg .f64 	%fd<484>;


	mov.u64 	%SPL, __local_depot4;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r41, [calcRayTan_param_0];
	ld.param.u32 	%r42, [calcRayTan_param_1];
	ld.param.u32 	%r43, [calcRayTan_param_4];
	ld.param.u32 	%r44, [calcRayTan_param_5];
	ld.param.f64 	%fd62, [calcRayTan_param_6];
	ld.param.u32 	%r45, [calcRayTan_param_7];
	ld.param.u32 	%r46, [calcRayTan_param_9];
	ld.param.f32 	%f4, [calcRayTan_param_10];
	ld.param.u64 	%rd8, [calcRayTan_param_11];
	ld.param.u32 	%r47, [calcRayTan_param_12];
	ld.param.f64 	%fd63, [calcRayTan_param_13];
	ld.param.f64 	%fd64, [calcRayTan_param_14];
	ld.param.f64 	%fd65, [calcRayTan_param_15];
	ld.param.f64 	%fd66, [calcRayTan_param_16];
	ld.param.f64 	%fd67, [calcRayTan_param_17];
	ld.param.f64 	%fd68, [calcRayTan_param_18];
	ld.param.f64 	%fd69, [calcRayTan_param_19];
	cvta.to.global.u64 	%rd1, %rd8;
	add.u64 	%rd9, %SPL, 0;
	add.u64 	%rd10, %SP, 4;
	cvta.to.local.u64 	%rd3, %rd10;
	mov.u32 	%r48, %ntid.x;
	mov.u32 	%r49, %ctaid.x;
	mov.u32 	%r50, %tid.x;
	mad.lo.s32 	%r1, %r48, %r49, %r50;
	setp.ge.s32	%p1, %r1, %r47;
	@%p1 bra 	BB4_57;

	cvt.rn.f64.s32	%fd70, %r1;
	mul.f64 	%fd71, %fd70, %fd63;
	sub.f64 	%fd72, %fd66, %fd71;
	setp.lt.f64	%p2, %fd72, 0d0000000000000000;
	add.f64 	%fd73, %fd72, 0d401921FB54442D18;
	selp.f64	%fd1, %fd73, %fd72, %p2;
	setp.ge.f64	%p3, %fd1, %fd67;
	setp.lt.f64	%p4, %fd67, %fd66;
	and.pred  	%p5, %p4, %p3;
	setp.le.f64	%p6, %fd1, %fd66;
	and.pred  	%p7, %p5, %p6;
	@%p7 bra 	BB4_3;

	or.pred  	%p10, %p3, %p6;
	setp.ge.f64	%p11, %fd67, %fd66;
	and.pred  	%p12, %p11, %p10;
	@!%p12 bra 	BB4_57;
	bra.uni 	BB4_3;

BB4_3:
	setp.lt.f64	%p13, %fd1, 0d400921FB54442D18;
	setp.ge.f64	%p14, %fd1, 0d0000000000000000;
	and.pred  	%p15, %p14, %p13;
	add.s32 	%r51, %r44, -1;
	selp.b32	%r2, 0, %r51, %p15;
	setp.ge.f64	%p16, %fd1, 0d3FF921FB54442D18;
	setp.lt.f64	%p17, %fd1, 0d4012D97C7F3321D2;
	and.pred  	%p18, %p16, %p17;
	add.s32 	%r52, %r43, -1;
	selp.b32	%r141, 0, %r52, %p18;
	setp.eq.s32	%p19, %r141, 0;
	selp.b32	%r4, -1, 1, %p19;
	abs.f64 	%fd74, %fd1;
	setp.neu.f64	%p20, %fd74, 0d7FF0000000000000;
	mov.f64 	%fd460, %fd1;
	@%p20 bra 	BB4_5;

	mov.f64 	%fd75, 0d0000000000000000;
	mul.rn.f64 	%fd2, %fd1, %fd75;
	mov.f64 	%fd460, %fd2;

BB4_5:
	mov.f64 	%fd3, %fd460;
	mul.f64 	%fd76, %fd3, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r138, %fd76;
	st.local.u32 	[%rd3], %r138;
	cvt.rn.f64.s32	%fd77, %r138;
	neg.f64 	%fd78, %fd77;
	mov.f64 	%fd79, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd80, %fd78, %fd79, %fd3;
	mov.f64 	%fd81, 0d3C91A62633145C00;
	fma.rn.f64 	%fd82, %fd78, %fd81, %fd80;
	mov.f64 	%fd83, 0d397B839A252049C0;
	fma.rn.f64 	%fd461, %fd78, %fd83, %fd82;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r53}, %fd3;
	}
	and.b32  	%r54, %r53, 2145386496;
	setp.lt.u32	%p21, %r54, 1105199104;
	@%p21 bra 	BB4_7;

	// Callseq Start 0
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd10;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd461, [retval0+0];
	}
	// Callseq End 0
	ld.local.u32 	%r138, [%rd3];

BB4_7:
	mul.f64 	%fd84, %fd461, %fd461;
	mov.f64 	%fd85, 0dBEF9757C5B27EBB1;
	mov.f64 	%fd86, 0d3EE48DAC2799BCB9;
	fma.rn.f64 	%fd87, %fd86, %fd84, %fd85;
	mov.f64 	%fd88, 0d3F0980E90FD91E04;
	fma.rn.f64 	%fd89, %fd87, %fd84, %fd88;
	mov.f64 	%fd90, 0dBEFAE2B0417D7E1D;
	fma.rn.f64 	%fd91, %fd89, %fd84, %fd90;
	mov.f64 	%fd92, 0d3F119F5341BFBA57;
	fma.rn.f64 	%fd93, %fd91, %fd84, %fd92;
	mov.f64 	%fd94, 0d3F15E791A00F6919;
	fma.rn.f64 	%fd95, %fd93, %fd84, %fd94;
	mov.f64 	%fd96, 0d3F2FF2E7FADEC73A;
	fma.rn.f64 	%fd97, %fd95, %fd84, %fd96;
	mov.f64 	%fd98, 0d3F434BC1B206DA62;
	fma.rn.f64 	%fd99, %fd97, %fd84, %fd98;
	mov.f64 	%fd100, 0d3F57DB18EF2F83F9;
	fma.rn.f64 	%fd101, %fd99, %fd84, %fd100;
	mov.f64 	%fd102, 0d3F6D6D2E7AE49FBC;
	fma.rn.f64 	%fd103, %fd101, %fd84, %fd102;
	mov.f64 	%fd104, 0d3F8226E3A816A776;
	fma.rn.f64 	%fd105, %fd103, %fd84, %fd104;
	mov.f64 	%fd106, 0d3F9664F485D25660;
	fma.rn.f64 	%fd107, %fd105, %fd84, %fd106;
	mov.f64 	%fd108, 0d3FABA1BA1BABF31D;
	fma.rn.f64 	%fd109, %fd107, %fd84, %fd108;
	mov.f64 	%fd110, 0d3FC11111111105D2;
	fma.rn.f64 	%fd111, %fd109, %fd84, %fd110;
	mov.f64 	%fd112, 0d3FD555555555555E;
	fma.rn.f64 	%fd113, %fd111, %fd84, %fd112;
	mul.f64 	%fd7, %fd113, %fd84;
	fma.rn.f64 	%fd462, %fd7, %fd461, %fd461;
	and.b32  	%r55, %r138, 1;
	setp.eq.b32	%p22, %r55, 1;
	@!%p22 bra 	BB4_9;
	bra.uni 	BB4_8;

BB4_8:
	sub.f64 	%fd116, %fd462, %fd461;
	neg.f64 	%fd117, %fd116;
	fma.rn.f64 	%fd118, %fd7, %fd461, %fd117;
	// inline asm
	rcp.approx.ftz.f64 %fd114,%fd462;
	// inline asm
	neg.f64 	%fd119, %fd462;
	mov.f64 	%fd120, 0d3FF0000000000000;
	fma.rn.f64 	%fd121, %fd119, %fd114, %fd120;
	fma.rn.f64 	%fd122, %fd121, %fd121, %fd121;
	fma.rn.f64 	%fd123, %fd122, %fd114, %fd114;
	neg.f64 	%fd124, %fd123;
	fma.rn.f64 	%fd125, %fd462, %fd124, %fd120;
	fma.rn.f64 	%fd126, %fd124, %fd118, %fd125;
	fma.rn.f64 	%fd462, %fd126, %fd124, %fd124;

BB4_9:
	sub.s32 	%r56, %r141, %r41;
	abs.s32 	%r57, %r56;
	cvt.rn.f64.s32	%fd127, %r57;
	mul.f64 	%fd463, %fd462, %fd127;
	abs.f64 	%fd12, %fd463;
	setp.ge.f64	%p23, %fd12, 0d4330000000000000;
	@%p23 bra 	BB4_11;

	add.f64 	%fd128, %fd12, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd129, %fd128;
	setp.lt.f64	%p24, %fd12, 0d3FE0000000000000;
	selp.f64	%fd130, 0d0000000000000000, %fd129, %p24;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r58, %temp}, %fd130;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r59}, %fd130;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r60}, %fd463;
	}
	and.b32  	%r61, %r60, -2147483648;
	or.b32  	%r62, %r59, %r61;
	mov.b64 	%fd463, {%r58, %r62};

BB4_11:
	ld.param.u32 	%r136, [calcRayTan_param_5];
	neg.f64 	%fd131, %fd463;
	cvt.rzi.s32.f64	%r63, %fd131;
	mad.lo.s32 	%r8, %r63, %r4, %r42;
	setp.gt.s32	%p25, %r8, -1;
	setp.lt.s32	%p26, %r8, %r136;
	and.pred  	%p27, %p25, %p26;
	mov.u32 	%r140, %r8;
	@%p27 bra 	BB4_21;

	add.f64 	%fd464, %fd1, 0d3FF921FB54442D18;
	abs.f64 	%fd132, %fd464;
	setp.neu.f64	%p28, %fd132, 0d7FF0000000000000;
	@%p28 bra 	BB4_14;

	mov.f64 	%fd133, 0d0000000000000000;
	mul.rn.f64 	%fd464, %fd464, %fd133;

BB4_14:
	mov.f64 	%fd443, 0d397B839A252049C0;
	mov.f64 	%fd442, 0d3C91A62633145C00;
	mul.f64 	%fd134, %fd464, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r139, %fd134;
	st.local.u32 	[%rd9], %r139;
	cvt.rn.f64.s32	%fd135, %r139;
	neg.f64 	%fd136, %fd135;
	fma.rn.f64 	%fd138, %fd136, %fd79, %fd464;
	fma.rn.f64 	%fd140, %fd136, %fd442, %fd138;
	fma.rn.f64 	%fd465, %fd136, %fd443, %fd140;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r64}, %fd464;
	}
	and.b32  	%r65, %r64, 2145386496;
	setp.lt.u32	%p29, %r65, 1105199104;
	@%p29 bra 	BB4_16;

	add.u64 	%rd29, %SP, 0;
	// Callseq Start 1
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd464;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd465, [retval0+0];
	}
	// Callseq End 1
	ld.local.u32 	%r139, [%rd9];

BB4_16:
	mov.f64 	%fd458, 0d3FD555555555555E;
	mov.f64 	%fd457, 0d3FC11111111105D2;
	mov.f64 	%fd456, 0d3FABA1BA1BABF31D;
	mov.f64 	%fd455, 0d3F9664F485D25660;
	mov.f64 	%fd454, 0d3F8226E3A816A776;
	mov.f64 	%fd453, 0d3F6D6D2E7AE49FBC;
	mov.f64 	%fd452, 0d3F57DB18EF2F83F9;
	mov.f64 	%fd451, 0d3F434BC1B206DA62;
	mov.f64 	%fd450, 0d3F2FF2E7FADEC73A;
	mov.f64 	%fd449, 0d3F15E791A00F6919;
	mov.f64 	%fd448, 0d3F119F5341BFBA57;
	mov.f64 	%fd447, 0dBEFAE2B0417D7E1D;
	mov.f64 	%fd446, 0d3F0980E90FD91E04;
	mov.f64 	%fd445, 0dBEF9757C5B27EBB1;
	mov.f64 	%fd444, 0d3EE48DAC2799BCB9;
	mul.f64 	%fd142, %fd465, %fd465;
	fma.rn.f64 	%fd145, %fd444, %fd142, %fd445;
	fma.rn.f64 	%fd147, %fd145, %fd142, %fd446;
	fma.rn.f64 	%fd149, %fd147, %fd142, %fd447;
	fma.rn.f64 	%fd151, %fd149, %fd142, %fd448;
	fma.rn.f64 	%fd153, %fd151, %fd142, %fd449;
	fma.rn.f64 	%fd155, %fd153, %fd142, %fd450;
	fma.rn.f64 	%fd157, %fd155, %fd142, %fd451;
	fma.rn.f64 	%fd159, %fd157, %fd142, %fd452;
	fma.rn.f64 	%fd161, %fd159, %fd142, %fd453;
	fma.rn.f64 	%fd163, %fd161, %fd142, %fd454;
	fma.rn.f64 	%fd165, %fd163, %fd142, %fd455;
	fma.rn.f64 	%fd167, %fd165, %fd142, %fd456;
	fma.rn.f64 	%fd169, %fd167, %fd142, %fd457;
	fma.rn.f64 	%fd171, %fd169, %fd142, %fd458;
	mul.f64 	%fd21, %fd171, %fd142;
	fma.rn.f64 	%fd466, %fd21, %fd465, %fd465;
	and.b32  	%r66, %r139, 1;
	setp.eq.b32	%p30, %r66, 1;
	@!%p30 bra 	BB4_18;
	bra.uni 	BB4_17;

BB4_17:
	sub.f64 	%fd174, %fd466, %fd465;
	neg.f64 	%fd175, %fd174;
	fma.rn.f64 	%fd176, %fd21, %fd465, %fd175;
	// inline asm
	rcp.approx.ftz.f64 %fd172,%fd466;
	// inline asm
	neg.f64 	%fd177, %fd466;
	mov.f64 	%fd178, 0d3FF0000000000000;
	fma.rn.f64 	%fd179, %fd177, %fd172, %fd178;
	fma.rn.f64 	%fd180, %fd179, %fd179, %fd179;
	fma.rn.f64 	%fd181, %fd180, %fd172, %fd172;
	neg.f64 	%fd182, %fd181;
	fma.rn.f64 	%fd183, %fd466, %fd182, %fd178;
	fma.rn.f64 	%fd184, %fd182, %fd176, %fd183;
	fma.rn.f64 	%fd466, %fd184, %fd182, %fd182;

BB4_18:
	sub.s32 	%r67, %r2, %r42;
	abs.s32 	%r68, %r67;
	cvt.rn.f64.s32	%fd185, %r68;
	mul.f64 	%fd467, %fd466, %fd185;
	abs.f64 	%fd26, %fd467;
	setp.ge.f64	%p31, %fd26, 0d4330000000000000;
	@%p31 bra 	BB4_20;

	add.f64 	%fd186, %fd26, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd187, %fd186;
	setp.lt.f64	%p32, %fd26, 0d3FE0000000000000;
	selp.f64	%fd188, 0d0000000000000000, %fd187, %p32;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r69, %temp}, %fd188;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r70}, %fd188;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r71}, %fd467;
	}
	and.b32  	%r72, %r71, -2147483648;
	or.b32  	%r73, %r70, %r72;
	mov.b64 	%fd467, {%r69, %r73};

BB4_20:
	abs.f64 	%fd189, %fd467;
	cvt.rzi.s32.f64	%r74, %fd189;
	mad.lo.s32 	%r141, %r74, %r4, %r41;
	mov.u32 	%r140, %r2;

BB4_21:
	ld.param.u32 	%r137, [calcRayTan_param_5];
	setp.ge.s32	%p33, %r141, %r43;
	setp.lt.s32	%p34, %r141, 0;
	or.pred  	%p35, %p34, %p33;
	setp.lt.s32	%p36, %r140, 0;
	or.pred  	%p37, %p35, %p36;
	setp.ge.s32	%p38, %r140, %r137;
	or.pred  	%p39, %p37, %p38;
	@%p39 bra 	BB4_54;

	ld.param.f64 	%fd459, [calcRayTan_param_2];
	ld.param.u64 	%rd30, [calcRayTan_param_3];
	cvta.to.global.u64 	%rd13, %rd30;
	sub.s32 	%r75, %r141, %r41;
	abs.s32 	%r15, %r75;
	sub.s32 	%r76, %r140, %r42;
	abs.s32 	%r16, %r76;
	mad.lo.s32 	%r143, %r43, %r42, %r41;
	mad.lo.s32 	%r18, %r140, %r43, %r141;
	mul.wide.s32 	%rd14, %r143, 4;
	add.s64 	%rd15, %rd13, %rd14;
	ld.global.f32 	%f5, [%rd15];
	cvt.f64.f32	%fd190, %f5;
	add.f64 	%fd29, %fd190, %fd459;
	mul.f64 	%fd191, %fd62, 0dBFE0000000000000;
	div.rn.f64 	%fd30, %fd459, %fd191;
	setp.neu.f64	%p40, %fd64, 0d0000000000000000;
	@%p40 bra 	BB4_29;

	min.f64 	%fd31, %fd30, %fd69;
	abs.f64 	%fd32, %fd31;
	setp.leu.f64	%p41, %fd32, 0d3FF0000000000000;
	mov.f64 	%fd468, %fd32;
	@%p41 bra 	BB4_25;

	// inline asm
	rcp.approx.ftz.f64 %fd192,%fd32;
	// inline asm
	neg.f64 	%fd194, %fd32;
	mov.f64 	%fd195, 0d3FF0000000000000;
	fma.rn.f64 	%fd196, %fd194, %fd192, %fd195;
	fma.rn.f64 	%fd197, %fd196, %fd196, %fd196;
	fma.rn.f64 	%fd198, %fd197, %fd192, %fd192;
	setp.eq.f64	%p42, %fd32, 0d7FF0000000000000;
	selp.f64	%fd33, 0d0000000000000000, %fd198, %p42;
	mov.f64 	%fd468, %fd33;

BB4_25:
	mov.f64 	%fd34, %fd468;
	mul.f64 	%fd199, %fd34, %fd34;
	mov.f64 	%fd200, 0d3F2D3B63DBB65B49;
	mov.f64 	%fd201, 0dBEF53E1D2A25FF7E;
	fma.rn.f64 	%fd202, %fd201, %fd199, %fd200;
	mov.f64 	%fd203, 0dBF5312788DDE082E;
	fma.rn.f64 	%fd204, %fd202, %fd199, %fd203;
	mov.f64 	%fd205, 0d3F6F9690C8249315;
	fma.rn.f64 	%fd206, %fd204, %fd199, %fd205;
	mov.f64 	%fd207, 0dBF82CF5AABC7CF0D;
	fma.rn.f64 	%fd208, %fd206, %fd199, %fd207;
	mov.f64 	%fd209, 0d3F9162B0B2A3BFDE;
	fma.rn.f64 	%fd210, %fd208, %fd199, %fd209;
	mov.f64 	%fd211, 0dBF9A7256FEB6FC6B;
	fma.rn.f64 	%fd212, %fd210, %fd199, %fd211;
	mov.f64 	%fd213, 0d3FA171560CE4A489;
	fma.rn.f64 	%fd214, %fd212, %fd199, %fd213;
	mov.f64 	%fd215, 0dBFA4F44D841450E4;
	fma.rn.f64 	%fd216, %fd214, %fd199, %fd215;
	mov.f64 	%fd217, 0d3FA7EE3D3F36BB95;
	fma.rn.f64 	%fd218, %fd216, %fd199, %fd217;
	mov.f64 	%fd219, 0dBFAAD32AE04A9FD1;
	fma.rn.f64 	%fd220, %fd218, %fd199, %fd219;
	mov.f64 	%fd221, 0d3FAE17813D66954F;
	fma.rn.f64 	%fd222, %fd220, %fd199, %fd221;
	mov.f64 	%fd223, 0dBFB11089CA9A5BCD;
	fma.rn.f64 	%fd224, %fd222, %fd199, %fd223;
	mov.f64 	%fd225, 0d3FB3B12B2DB51738;
	fma.rn.f64 	%fd226, %fd224, %fd199, %fd225;
	mov.f64 	%fd227, 0dBFB745D022F8DC5C;
	fma.rn.f64 	%fd228, %fd226, %fd199, %fd227;
	mov.f64 	%fd229, 0d3FBC71C709DFE927;
	fma.rn.f64 	%fd230, %fd228, %fd199, %fd229;
	mov.f64 	%fd231, 0dBFC2492491FA1744;
	fma.rn.f64 	%fd232, %fd230, %fd199, %fd231;
	mov.f64 	%fd233, 0d3FC99999999840D2;
	fma.rn.f64 	%fd234, %fd232, %fd199, %fd233;
	mov.f64 	%fd235, 0dBFD555555555544C;
	fma.rn.f64 	%fd236, %fd234, %fd199, %fd235;
	mul.f64 	%fd237, %fd236, %fd199;
	fma.rn.f64 	%fd238, %fd237, %fd34, %fd34;
	sub.f64 	%fd240, %fd79, %fd238;
	setp.gt.f64	%p43, %fd32, 0d3FF0000000000000;
	selp.f64	%fd241, %fd240, %fd238, %p43;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r77, %temp}, %fd241;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r78}, %fd241;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r79}, %fd31;
	}
	and.b32  	%r80, %r79, -2147483648;
	or.b32  	%r81, %r78, %r80;
	mov.b64 	%fd242, {%r77, %r81};
	sub.f64 	%fd243, %fd79, %fd242;
	div.rn.f64 	%fd244, %fd243, %fd63;
	cvt.rzi.s32.f64	%r142, %fd244;
	abs.f64 	%fd35, %fd68;
	setp.leu.f64	%p44, %fd35, 0d3FF0000000000000;
	mov.f64 	%fd469, %fd35;
	@%p44 bra 	BB4_27;

	// inline asm
	rcp.approx.ftz.f64 %fd245,%fd35;
	// inline asm
	neg.f64 	%fd247, %fd35;
	mov.f64 	%fd248, 0d3FF0000000000000;
	fma.rn.f64 	%fd249, %fd247, %fd245, %fd248;
	fma.rn.f64 	%fd250, %fd249, %fd249, %fd249;
	fma.rn.f64 	%fd251, %fd250, %fd245, %fd245;
	setp.eq.f64	%p45, %fd35, 0d7FF0000000000000;
	selp.f64	%fd36, 0d0000000000000000, %fd251, %p45;
	mov.f64 	%fd469, %fd36;

BB4_27:
	mov.f64 	%fd37, %fd469;
	mul.f64 	%fd252, %fd37, %fd37;
	fma.rn.f64 	%fd255, %fd201, %fd252, %fd200;
	fma.rn.f64 	%fd257, %fd255, %fd252, %fd203;
	fma.rn.f64 	%fd259, %fd257, %fd252, %fd205;
	fma.rn.f64 	%fd261, %fd259, %fd252, %fd207;
	fma.rn.f64 	%fd263, %fd261, %fd252, %fd209;
	fma.rn.f64 	%fd265, %fd263, %fd252, %fd211;
	fma.rn.f64 	%fd267, %fd265, %fd252, %fd213;
	fma.rn.f64 	%fd269, %fd267, %fd252, %fd215;
	fma.rn.f64 	%fd271, %fd269, %fd252, %fd217;
	fma.rn.f64 	%fd273, %fd271, %fd252, %fd219;
	fma.rn.f64 	%fd275, %fd273, %fd252, %fd221;
	fma.rn.f64 	%fd277, %fd275, %fd252, %fd223;
	fma.rn.f64 	%fd279, %fd277, %fd252, %fd225;
	fma.rn.f64 	%fd281, %fd279, %fd252, %fd227;
	fma.rn.f64 	%fd283, %fd281, %fd252, %fd229;
	fma.rn.f64 	%fd285, %fd283, %fd252, %fd231;
	fma.rn.f64 	%fd287, %fd285, %fd252, %fd233;
	fma.rn.f64 	%fd289, %fd287, %fd252, %fd235;
	mul.f64 	%fd290, %fd289, %fd252;
	fma.rn.f64 	%fd291, %fd290, %fd37, %fd37;
	sub.f64 	%fd293, %fd79, %fd291;
	setp.gt.f64	%p46, %fd35, 0d3FF0000000000000;
	selp.f64	%fd294, %fd293, %fd291, %p46;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r82, %temp}, %fd294;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r83}, %fd294;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r84}, %fd68;
	}
	and.b32  	%r85, %r84, -2147483648;
	or.b32  	%r86, %r83, %r85;
	mov.b64 	%fd295, {%r82, %r86};
	sub.f64 	%fd296, %fd79, %fd295;
	div.rn.f64 	%fd297, %fd296, %fd63;
	cvt.rzi.s32.f64	%r20, %fd297;
	setp.ge.s32	%p47, %r142, %r20;
	@%p47 bra 	BB4_29;

BB4_28:
	mad.lo.s32 	%r87, %r142, %r47, %r1;
	mul.wide.s32 	%rd16, %r87, 4;
	add.s64 	%rd17, %rd1, %rd16;
	st.global.u32 	[%rd17], %r143;
	add.s32 	%r142, %r142, 1;
	setp.lt.s32	%p48, %r142, %r20;
	@%p48 bra 	BB4_28;

BB4_29:
	setp.eq.s32	%p49, %r143, %r18;
	@%p49 bra 	BB4_57;

	ld.param.u64 	%rd28, [calcRayTan_param_8];
	ld.param.u32 	%r135, [calcRayTan_param_1];
	ld.param.u32 	%r134, [calcRayTan_param_0];
	setp.gt.s32	%p50, %r141, %r134;
	selp.b32	%r23, 1, -1, %p50;
	setp.gt.s32	%p51, %r140, %r135;
	selp.b32	%r24, 1, -1, %p51;
	sub.s32 	%r146, %r15, %r16;
	max.f64 	%fd483, %fd30, %fd68;
	mov.u32 	%r145, 0;
	mov.u32 	%r144, %r145;
	mov.f64 	%fd470, 0dDA4D8BA7F519C84F;
	cvta.to.global.u64 	%rd21, %rd28;

BB4_31:
	mov.f64 	%fd478, %fd483;
	mov.f64 	%fd40, %fd478;
	neg.s32 	%r90, %r16;
	shl.b32 	%r91, %r146, 1;
	setp.gt.s32	%p52, %r91, %r90;
	selp.b32	%r92, %r16, 0, %p52;
	sub.s32 	%r93, %r146, %r92;
	selp.b32	%r94, %r23, 0, %p52;
	add.s32 	%r145, %r94, %r145;
	setp.lt.s32	%p53, %r91, %r15;
	selp.b32	%r95, %r15, 0, %p53;
	add.s32 	%r146, %r93, %r95;
	selp.b32	%r96, %r24, 0, %p53;
	add.s32 	%r144, %r96, %r144;
	mul.lo.s32 	%r97, %r24, %r43;
	selp.b32	%r98, %r97, 0, %p53;
	add.s32 	%r99, %r94, %r98;
	add.s32 	%r143, %r99, %r143;
	cvt.s64.s32	%rd4, %r143;
	mul.wide.s32 	%rd19, %r143, 4;
	add.s64 	%rd20, %rd13, %rd19;
	ld.global.f32 	%f1, [%rd20];
	setp.ne.s32	%p54, %r45, 0;
	@%p54 bra 	BB4_33;

	mov.f32 	%f10, 0f00000000;
	bra.uni 	BB4_34;

BB4_33:
	shl.b64 	%rd22, %rd4, 2;
	add.s64 	%rd23, %rd21, %rd22;
	ld.global.f32 	%f10, [%rd23];

BB4_34:
	add.f32 	%f7, %f1, %f10;
	cvt.f64.f32	%fd471, %f7;
	abs.f64 	%fd299, %fd471;
	setp.gtu.f64	%p55, %fd299, 0d7FF0000000000000;
	@%p55 bra 	BB4_57;

	setp.le.f64	%p56, %fd471, %fd470;
	setp.ge.f64	%p57, %fd40, 0d0000000000000000;
	and.pred  	%p58, %p57, %p56;
	mov.f64 	%fd482, %fd40;
	@%p58 bra 	BB4_53;

	mul.lo.s32 	%r100, %r144, %r144;
	mad.lo.s32 	%r101, %r145, %r145, %r100;
	cvt.rn.f64.s32	%fd300, %r101;
	sqrt.rn.f64 	%fd301, %fd300;
	sub.f64 	%fd302, %fd471, %fd29;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r102}, %fd302;
	}
	and.b32  	%r103, %r102, -2147483648;
	mov.f64 	%fd303, 0d3FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r104}, %fd303;
	}
	and.b32  	%r105, %r104, 2147483647;
	or.b32  	%r106, %r105, %r103;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r107, %temp}, %fd303;
	}
	mov.b64 	%fd304, {%r107, %r106};
	mul.f64 	%fd305, %fd304, %fd62;
	mul.f64 	%fd306, %fd305, 0dBFE0000000000000;
	fma.rn.f64 	%fd42, %fd301, %fd62, %fd306;
	setp.gt.f64	%p59, %fd42, %fd65;
	@%p59 bra 	BB4_57;

	setp.eq.s32	%p60, %r46, 0;
	@%p60 bra 	BB4_39;

	mov.f32 	%f8, 0f3F800000;
	sub.f32 	%f9, %f8, %f4;
	cvt.f64.f32	%fd307, %f9;
	mul.f64 	%fd308, %fd307, %fd42;
	mul.f64 	%fd309, %fd308, %fd42;
	div.rn.f64 	%fd310, %fd309, 0dC1684CB400000000;
	add.f64 	%fd471, %fd471, %fd310;

BB4_39:
	sub.f64 	%fd311, %fd471, %fd29;
	div.rn.f64 	%fd45, %fd311, %fd42;
	setp.leu.f64	%p61, %fd45, %fd40;
	mov.f64 	%fd481, %fd40;
	@%p61 bra 	BB4_51;

	setp.ltu.f64	%p62, %fd42, %fd64;
	mov.f64 	%fd481, %fd45;
	@%p62 bra 	BB4_51;

	abs.f64 	%fd47, %fd40;
	setp.leu.f64	%p63, %fd47, 0d3FF0000000000000;
	mov.f64 	%fd472, %fd47;
	@%p63 bra 	BB4_43;

	// inline asm
	rcp.approx.ftz.f64 %fd312,%fd47;
	// inline asm
	neg.f64 	%fd314, %fd47;
	fma.rn.f64 	%fd316, %fd314, %fd312, %fd303;
	fma.rn.f64 	%fd317, %fd316, %fd316, %fd316;
	fma.rn.f64 	%fd318, %fd317, %fd312, %fd312;
	setp.eq.f64	%p64, %fd47, 0d7FF0000000000000;
	selp.f64	%fd48, 0d0000000000000000, %fd318, %p64;
	mov.f64 	%fd472, %fd48;

BB4_43:
	mov.f64 	%fd49, %fd472;
	mul.f64 	%fd319, %fd49, %fd49;
	mov.f64 	%fd320, 0d3F2D3B63DBB65B49;
	mov.f64 	%fd321, 0dBEF53E1D2A25FF7E;
	fma.rn.f64 	%fd322, %fd321, %fd319, %fd320;
	mov.f64 	%fd323, 0dBF5312788DDE082E;
	fma.rn.f64 	%fd324, %fd322, %fd319, %fd323;
	mov.f64 	%fd325, 0d3F6F9690C8249315;
	fma.rn.f64 	%fd326, %fd324, %fd319, %fd325;
	mov.f64 	%fd327, 0dBF82CF5AABC7CF0D;
	fma.rn.f64 	%fd328, %fd326, %fd319, %fd327;
	mov.f64 	%fd329, 0d3F9162B0B2A3BFDE;
	fma.rn.f64 	%fd330, %fd328, %fd319, %fd329;
	mov.f64 	%fd331, 0dBF9A7256FEB6FC6B;
	fma.rn.f64 	%fd332, %fd330, %fd319, %fd331;
	mov.f64 	%fd333, 0d3FA171560CE4A489;
	fma.rn.f64 	%fd334, %fd332, %fd319, %fd333;
	mov.f64 	%fd335, 0dBFA4F44D841450E4;
	fma.rn.f64 	%fd336, %fd334, %fd319, %fd335;
	mov.f64 	%fd337, 0d3FA7EE3D3F36BB95;
	fma.rn.f64 	%fd338, %fd336, %fd319, %fd337;
	mov.f64 	%fd339, 0dBFAAD32AE04A9FD1;
	fma.rn.f64 	%fd340, %fd338, %fd319, %fd339;
	mov.f64 	%fd341, 0d3FAE17813D66954F;
	fma.rn.f64 	%fd342, %fd340, %fd319, %fd341;
	mov.f64 	%fd343, 0dBFB11089CA9A5BCD;
	fma.rn.f64 	%fd344, %fd342, %fd319, %fd343;
	mov.f64 	%fd345, 0d3FB3B12B2DB51738;
	fma.rn.f64 	%fd346, %fd344, %fd319, %fd345;
	mov.f64 	%fd347, 0dBFB745D022F8DC5C;
	fma.rn.f64 	%fd348, %fd346, %fd319, %fd347;
	mov.f64 	%fd349, 0d3FBC71C709DFE927;
	fma.rn.f64 	%fd350, %fd348, %fd319, %fd349;
	mov.f64 	%fd351, 0dBFC2492491FA1744;
	fma.rn.f64 	%fd352, %fd350, %fd319, %fd351;
	mov.f64 	%fd353, 0d3FC99999999840D2;
	fma.rn.f64 	%fd354, %fd352, %fd319, %fd353;
	mov.f64 	%fd355, 0dBFD555555555544C;
	fma.rn.f64 	%fd356, %fd354, %fd319, %fd355;
	mul.f64 	%fd357, %fd356, %fd319;
	fma.rn.f64 	%fd358, %fd357, %fd49, %fd49;
	sub.f64 	%fd360, %fd79, %fd358;
	setp.gt.f64	%p65, %fd47, 0d3FF0000000000000;
	selp.f64	%fd361, %fd360, %fd358, %p65;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r108, %temp}, %fd361;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r109}, %fd361;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r110}, %fd40;
	}
	and.b32  	%r111, %r110, -2147483648;
	or.b32  	%r112, %r109, %r111;
	mov.b64 	%fd362, {%r108, %r112};
	sub.f64 	%fd363, %fd79, %fd362;
	div.rn.f64 	%fd473, %fd363, %fd63;
	abs.f64 	%fd51, %fd473;
	setp.ge.f64	%p66, %fd51, 0d4330000000000000;
	@%p66 bra 	BB4_45;

	add.f64 	%fd364, %fd51, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd365, %fd364;
	setp.lt.f64	%p67, %fd51, 0d3FE0000000000000;
	selp.f64	%fd366, 0d0000000000000000, %fd365, %p67;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r113, %temp}, %fd366;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r114}, %fd366;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r115}, %fd473;
	}
	and.b32  	%r116, %r115, -2147483648;
	or.b32  	%r117, %r114, %r116;
	mov.b64 	%fd473, {%r113, %r117};

BB4_45:
	min.f64 	%fd420, %fd69, %fd45;
	cvt.rzi.s32.f64	%r34, %fd473;
	abs.f64 	%fd54, %fd420;
	setp.leu.f64	%p68, %fd54, 0d3FF0000000000000;
	mov.f64 	%fd474, %fd54;
	@%p68 bra 	BB4_47;

	mov.f64 	%fd441, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd367,%fd54;
	// inline asm
	neg.f64 	%fd369, %fd54;
	fma.rn.f64 	%fd371, %fd369, %fd367, %fd441;
	fma.rn.f64 	%fd372, %fd371, %fd371, %fd371;
	fma.rn.f64 	%fd373, %fd372, %fd367, %fd367;
	setp.eq.f64	%p69, %fd54, 0d7FF0000000000000;
	selp.f64	%fd55, 0d0000000000000000, %fd373, %p69;
	mov.f64 	%fd474, %fd55;

BB4_47:
	mov.f64 	%fd56, %fd474;
	mov.f64 	%fd440, 0dBFD555555555544C;
	mov.f64 	%fd439, 0d3FC99999999840D2;
	mov.f64 	%fd438, 0dBFC2492491FA1744;
	mov.f64 	%fd437, 0d3FBC71C709DFE927;
	mov.f64 	%fd436, 0dBFB745D022F8DC5C;
	mov.f64 	%fd435, 0d3FB3B12B2DB51738;
	mov.f64 	%fd434, 0dBFB11089CA9A5BCD;
	mov.f64 	%fd433, 0d3FAE17813D66954F;
	mov.f64 	%fd432, 0dBFAAD32AE04A9FD1;
	mov.f64 	%fd431, 0d3FA7EE3D3F36BB95;
	mov.f64 	%fd430, 0dBFA4F44D841450E4;
	mov.f64 	%fd429, 0d3FA171560CE4A489;
	mov.f64 	%fd428, 0dBF9A7256FEB6FC6B;
	mov.f64 	%fd427, 0d3F9162B0B2A3BFDE;
	mov.f64 	%fd426, 0dBF82CF5AABC7CF0D;
	mov.f64 	%fd425, 0d3F6F9690C8249315;
	mov.f64 	%fd424, 0dBF5312788DDE082E;
	mov.f64 	%fd423, 0d3F2D3B63DBB65B49;
	mov.f64 	%fd422, 0dBEF53E1D2A25FF7E;
	min.f64 	%fd421, %fd69, %fd45;
	mul.f64 	%fd374, %fd56, %fd56;
	fma.rn.f64 	%fd377, %fd422, %fd374, %fd423;
	fma.rn.f64 	%fd379, %fd377, %fd374, %fd424;
	fma.rn.f64 	%fd381, %fd379, %fd374, %fd425;
	fma.rn.f64 	%fd383, %fd381, %fd374, %fd426;
	fma.rn.f64 	%fd385, %fd383, %fd374, %fd427;
	fma.rn.f64 	%fd387, %fd385, %fd374, %fd428;
	fma.rn.f64 	%fd389, %fd387, %fd374, %fd429;
	fma.rn.f64 	%fd391, %fd389, %fd374, %fd430;
	fma.rn.f64 	%fd393, %fd391, %fd374, %fd431;
	fma.rn.f64 	%fd395, %fd393, %fd374, %fd432;
	fma.rn.f64 	%fd397, %fd395, %fd374, %fd433;
	fma.rn.f64 	%fd399, %fd397, %fd374, %fd434;
	fma.rn.f64 	%fd401, %fd399, %fd374, %fd435;
	fma.rn.f64 	%fd403, %fd401, %fd374, %fd436;
	fma.rn.f64 	%fd405, %fd403, %fd374, %fd437;
	fma.rn.f64 	%fd407, %fd405, %fd374, %fd438;
	fma.rn.f64 	%fd409, %fd407, %fd374, %fd439;
	fma.rn.f64 	%fd411, %fd409, %fd374, %fd440;
	mul.f64 	%fd412, %fd411, %fd374;
	fma.rn.f64 	%fd413, %fd412, %fd56, %fd56;
	sub.f64 	%fd415, %fd79, %fd413;
	setp.gt.f64	%p70, %fd54, 0d3FF0000000000000;
	selp.f64	%fd416, %fd415, %fd413, %p70;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r118, %temp}, %fd416;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r119}, %fd416;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r120}, %fd421;
	}
	and.b32  	%r121, %r120, -2147483648;
	or.b32  	%r122, %r119, %r121;
	mov.b64 	%fd417, {%r118, %r122};
	sub.f64 	%fd418, %fd79, %fd417;
	div.rn.f64 	%fd419, %fd418, %fd63;
	cvt.rzi.s32.f64	%r147, %fd419;
	setp.ge.s32	%p71, %r147, %r34;
	mov.f64 	%fd476, %fd45;
	mov.f64 	%fd481, %fd476;
	@%p71 bra 	BB4_51;

BB4_48:
	mad.lo.s32 	%r127, %r147, %r47, %r1;
	mul.wide.s32 	%rd25, %r127, 4;
	add.s64 	%rd5, %rd1, %rd25;
	ld.global.u32 	%r128, [%rd5];
	setp.ne.s32	%p72, %r128, -1;
	@%p72 bra 	BB4_50;

	st.global.u32 	[%rd5], %r143;

BB4_50:
	add.s32 	%r147, %r147, 1;
	setp.lt.s32	%p73, %r147, %r34;
	mov.f64 	%fd475, %fd45;
	mov.f64 	%fd481, %fd475;
	@%p73 bra 	BB4_48;

BB4_51:
	mov.f64 	%fd482, %fd481;
	setp.gt.f64	%p74, %fd482, %fd69;
	@%p74 bra 	BB4_57;

	setp.gt.f64	%p75, %fd471, %fd470;
	selp.f64	%fd470, %fd471, %fd470, %p75;

BB4_53:
	mov.f64 	%fd483, %fd482;
	setp.eq.s32	%p76, %r143, %r18;
	@%p76 bra 	BB4_57;
	bra.uni 	BB4_31;

BB4_54:
	shr.u32 	%r129, %r47, 31;
	add.s32 	%r130, %r47, %r129;
	shr.s32 	%r38, %r130, 1;
	setp.lt.s32	%p77, %r47, 2;
	@%p77 bra 	BB4_57;

	mov.u32 	%r148, 0;

BB4_56:
	mad.lo.s32 	%r132, %r148, %r47, %r1;
	mul.wide.s32 	%rd26, %r132, 4;
	add.s64 	%rd27, %rd1, %rd26;
	mov.u32 	%r133, -1;
	st.global.u32 	[%rd27], %r133;
	add.s32 	%r148, %r148, 1;
	setp.lt.s32	%p78, %r148, %r38;
	@%p78 bra 	BB4_56;

BB4_57:
	ret;
}

.visible .entry sumView(
	.param .u64 sumView_param_0,
	.param .u32 sumView_param_1,
	.param .u64 sumView_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .s32 	%r<45>;
	.reg .s64 	%rd<13>;
	// demoted variable
	.shared .align 4 .b8 sumView$__cuda_local_var_36405_29_non_const_sdata[2048];

	ld.param.u64 	%rd5, [sumView_param_0];
	ld.param.u32 	%r9, [sumView_param_1];
	ld.param.u64 	%rd4, [sumView_param_2];
	cvta.to.global.u64 	%rd1, %rd5;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 9;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r43, %r10, %r2;
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 9;
	mul.wide.u32 	%rd6, %r2, 4;
	mov.u64 	%rd7, sumView$__cuda_local_var_36405_29_non_const_sdata;
	add.s64 	%rd2, %rd7, %rd6;
	mov.u32 	%r44, 0;
	st.shared.u32 	[%rd2], %r44;
	setp.ge.u32	%p1, %r43, %r9;
	@%p1 bra 	BB5_3;

BB5_1:
	cvt.u64.u32	%rd8, %r43;
	add.s64 	%rd9, %rd1, %rd8;
	ld.global.u8 	%r14, [%rd9];
	add.s32 	%r44, %r14, %r44;
	add.s32 	%r43, %r43, %r4;
	setp.lt.u32	%p2, %r43, %r9;
	@%p2 bra 	BB5_1;

	st.shared.u32 	[%rd2], %r44;

BB5_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r2, 255;
	@%p3 bra 	BB5_5;

	ld.shared.u32 	%r15, [%rd2];
	ld.shared.u32 	%r16, [%rd2+1024];
	add.s32 	%r17, %r15, %r16;
	st.shared.u32 	[%rd2], %r17;

BB5_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 127;
	@%p4 bra 	BB5_7;

	ld.shared.u32 	%r18, [%rd2];
	ld.shared.u32 	%r19, [%rd2+512];
	add.s32 	%r20, %r18, %r19;
	st.shared.u32 	[%rd2], %r20;

BB5_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 63;
	@%p5 bra 	BB5_9;

	ld.shared.u32 	%r21, [%rd2];
	ld.shared.u32 	%r22, [%rd2+256];
	add.s32 	%r23, %r21, %r22;
	st.shared.u32 	[%rd2], %r23;

BB5_9:
	bar.sync 	0;
	setp.gt.u32	%p6, %r2, 31;
	@%p6 bra 	BB5_11;

	ld.shared.u32 	%r24, [%rd2];
	ld.shared.u32 	%r25, [%rd2+128];
	add.s32 	%r26, %r24, %r25;
	st.shared.u32 	[%rd2], %r26;
	bar.sync 	0;
	ld.shared.u32 	%r27, [%rd2];
	ld.shared.u32 	%r28, [%rd2+64];
	add.s32 	%r29, %r27, %r28;
	st.shared.u32 	[%rd2], %r29;
	bar.sync 	0;
	ld.shared.u32 	%r30, [%rd2];
	ld.shared.u32 	%r31, [%rd2+32];
	add.s32 	%r32, %r30, %r31;
	st.shared.u32 	[%rd2], %r32;
	bar.sync 	0;
	ld.shared.u32 	%r33, [%rd2];
	ld.shared.u32 	%r34, [%rd2+16];
	add.s32 	%r35, %r33, %r34;
	st.shared.u32 	[%rd2], %r35;
	bar.sync 	0;
	ld.shared.u32 	%r36, [%rd2];
	ld.shared.u32 	%r37, [%rd2+8];
	add.s32 	%r38, %r36, %r37;
	st.shared.u32 	[%rd2], %r38;
	bar.sync 	0;
	ld.shared.u32 	%r39, [%rd2];
	ld.shared.u32 	%r40, [%rd2+4];
	add.s32 	%r41, %r39, %r40;
	st.shared.u32 	[%rd2], %r41;

BB5_11:
	setp.ne.s32	%p7, %r2, 0;
	@%p7 bra 	BB5_13;

	cvta.to.global.u64 	%rd10, %rd4;
	ld.shared.u32 	%r42, [sumView$__cuda_local_var_36405_29_non_const_sdata];
	mul.wide.u32 	%rd11, %r1, 4;
	add.s64 	%rd12, %rd10, %rd11;
	st.global.u32 	[%rd12], %r42;

BB5_13:
	ret;
}

.visible .entry sumViewTan(
	.param .u64 sumViewTan_param_0,
	.param .u32 sumViewTan_param_1,
	.param .u64 sumViewTan_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .s32 	%r<46>;
	.reg .s64 	%rd<13>;
	// demoted variable
	.shared .align 4 .b8 sumViewTan$__cuda_local_var_36435_29_non_const_sdata[2048];

	ld.param.u64 	%rd5, [sumViewTan_param_0];
	ld.param.u32 	%r10, [sumViewTan_param_1];
	ld.param.u64 	%rd4, [sumViewTan_param_2];
	cvta.to.global.u64 	%rd1, %rd5;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r11, %r1, 9;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r44, %r11, %r2;
	mov.u32 	%r12, %nctaid.x;
	shl.b32 	%r4, %r12, 9;
	mul.wide.u32 	%rd6, %r2, 4;
	mov.u64 	%rd7, sumViewTan$__cuda_local_var_36435_29_non_const_sdata;
	add.s64 	%rd2, %rd7, %rd6;
	mov.u32 	%r45, 0;
	st.shared.u32 	[%rd2], %r45;
	setp.ge.u32	%p1, %r44, %r10;
	@%p1 bra 	BB6_4;

BB6_1:
	mul.wide.u32 	%rd8, %r44, 4;
	add.s64 	%rd9, %rd1, %rd8;
	ld.global.u32 	%r15, [%rd9];
	setp.lt.s32	%p2, %r15, 0;
	@%p2 bra 	BB6_3;

	add.s32 	%r45, %r45, 1;
	st.shared.u32 	[%rd2], %r45;

BB6_3:
	add.s32 	%r44, %r44, %r4;
	setp.lt.u32	%p3, %r44, %r10;
	@%p3 bra 	BB6_1;

BB6_4:
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 255;
	@%p4 bra 	BB6_6;

	ld.shared.u32 	%r16, [%rd2];
	ld.shared.u32 	%r17, [%rd2+1024];
	add.s32 	%r18, %r16, %r17;
	st.shared.u32 	[%rd2], %r18;

BB6_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 127;
	@%p5 bra 	BB6_8;

	ld.shared.u32 	%r19, [%rd2];
	ld.shared.u32 	%r20, [%rd2+512];
	add.s32 	%r21, %r19, %r20;
	st.shared.u32 	[%rd2], %r21;

BB6_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r2, 63;
	@%p6 bra 	BB6_10;

	ld.shared.u32 	%r22, [%rd2];
	ld.shared.u32 	%r23, [%rd2+256];
	add.s32 	%r24, %r22, %r23;
	st.shared.u32 	[%rd2], %r24;

BB6_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r2, 31;
	@%p7 bra 	BB6_12;

	ld.shared.u32 	%r25, [%rd2];
	ld.shared.u32 	%r26, [%rd2+128];
	add.s32 	%r27, %r25, %r26;
	st.shared.u32 	[%rd2], %r27;
	bar.sync 	0;
	ld.shared.u32 	%r28, [%rd2];
	ld.shared.u32 	%r29, [%rd2+64];
	add.s32 	%r30, %r28, %r29;
	st.shared.u32 	[%rd2], %r30;
	bar.sync 	0;
	ld.shared.u32 	%r31, [%rd2];
	ld.shared.u32 	%r32, [%rd2+32];
	add.s32 	%r33, %r31, %r32;
	st.shared.u32 	[%rd2], %r33;
	bar.sync 	0;
	ld.shared.u32 	%r34, [%rd2];
	ld.shared.u32 	%r35, [%rd2+16];
	add.s32 	%r36, %r34, %r35;
	st.shared.u32 	[%rd2], %r36;
	bar.sync 	0;
	ld.shared.u32 	%r37, [%rd2];
	ld.shared.u32 	%r38, [%rd2+8];
	add.s32 	%r39, %r37, %r38;
	st.shared.u32 	[%rd2], %r39;
	bar.sync 	0;
	ld.shared.u32 	%r40, [%rd2];
	ld.shared.u32 	%r41, [%rd2+4];
	add.s32 	%r42, %r40, %r41;
	st.shared.u32 	[%rd2], %r42;

BB6_12:
	setp.ne.s32	%p8, %r2, 0;
	@%p8 bra 	BB6_14;

	cvta.to.global.u64 	%rd10, %rd4;
	ld.shared.u32 	%r43, [sumViewTan$__cuda_local_var_36435_29_non_const_sdata];
	mul.wide.u32 	%rd11, %r1, 4;
	add.s64 	%rd12, %rd10, %rd11;
	st.global.u32 	[%rd12], %r43;

BB6_14:
	ret;
}

.visible .entry sumLandView(
	.param .u64 sumLandView_param_0,
	.param .u32 sumLandView_param_1,
	.param .u64 sumLandView_param_2,
	.param .u8 sumLandView_param_3,
	.param .u64 sumLandView_param_4
)
{
	.reg .pred 	%p<9>;
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<47>;
	.reg .s64 	%rd<16>;
	// demoted variable
	.shared .align 4 .b8 sumLandView$__cuda_local_var_36469_29_non_const_sdata[2048];

	ld.param.u64 	%rd7, [sumLandView_param_0];
	ld.param.u32 	%r9, [sumLandView_param_1];
	ld.param.u64 	%rd8, [sumLandView_param_2];
	ld.param.u8 	%rs1, [sumLandView_param_3];
	ld.param.u64 	%rd6, [sumLandView_param_4];
	cvta.to.global.u64 	%rd1, %rd7;
	cvta.to.global.u64 	%rd2, %rd8;
	mov.u32 	%r10, %ctaid.x;
	shl.b32 	%r11, %r10, 9;
	mov.u32 	%r1, %tid.x;
	add.s32 	%r45, %r11, %r1;
	mov.u32 	%r12, %nctaid.x;
	shl.b32 	%r3, %r12, 9;
	mul.wide.u32 	%rd9, %r1, 4;
	mov.u64 	%rd10, sumLandView$__cuda_local_var_36469_29_non_const_sdata;
	add.s64 	%rd3, %rd10, %rd9;
	mov.u32 	%r46, 0;
	st.shared.u32 	[%rd3], %r46;
	setp.ge.u32	%p1, %r45, %r9;
	@%p1 bra 	BB7_4;

BB7_1:
	cvt.u64.u32	%rd4, %r45;
	add.s64 	%rd11, %rd2, %rd4;
	ld.global.u8 	%rs2, [%rd11];
	setp.ne.s16	%p2, %rs2, %rs1;
	@%p2 bra 	BB7_3;

	add.s64 	%rd12, %rd1, %rd4;
	ld.global.u8 	%r15, [%rd12];
	add.s32 	%r46, %r15, %r46;
	st.shared.u32 	[%rd3], %r46;

BB7_3:
	add.s32 	%r45, %r45, %r3;
	setp.lt.u32	%p3, %r45, %r9;
	@%p3 bra 	BB7_1;

BB7_4:
	bar.sync 	0;
	setp.gt.u32	%p4, %r1, 255;
	@%p4 bra 	BB7_6;

	ld.shared.u32 	%r16, [%rd3];
	ld.shared.u32 	%r17, [%rd3+1024];
	add.s32 	%r18, %r16, %r17;
	st.shared.u32 	[%rd3], %r18;

BB7_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r1, 127;
	@%p5 bra 	BB7_8;

	ld.shared.u32 	%r19, [%rd3];
	ld.shared.u32 	%r20, [%rd3+512];
	add.s32 	%r21, %r19, %r20;
	st.shared.u32 	[%rd3], %r21;

BB7_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r1, 63;
	@%p6 bra 	BB7_10;

	ld.shared.u32 	%r22, [%rd3];
	ld.shared.u32 	%r23, [%rd3+256];
	add.s32 	%r24, %r22, %r23;
	st.shared.u32 	[%rd3], %r24;

BB7_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r1, 31;
	@%p7 bra 	BB7_12;

	ld.shared.u32 	%r25, [%rd3];
	ld.shared.u32 	%r26, [%rd3+128];
	add.s32 	%r27, %r25, %r26;
	st.shared.u32 	[%rd3], %r27;
	bar.sync 	0;
	ld.shared.u32 	%r28, [%rd3];
	ld.shared.u32 	%r29, [%rd3+64];
	add.s32 	%r30, %r28, %r29;
	st.shared.u32 	[%rd3], %r30;
	bar.sync 	0;
	ld.shared.u32 	%r31, [%rd3];
	ld.shared.u32 	%r32, [%rd3+32];
	add.s32 	%r33, %r31, %r32;
	st.shared.u32 	[%rd3], %r33;
	bar.sync 	0;
	ld.shared.u32 	%r34, [%rd3];
	ld.shared.u32 	%r35, [%rd3+16];
	add.s32 	%r36, %r34, %r35;
	st.shared.u32 	[%rd3], %r36;
	bar.sync 	0;
	ld.shared.u32 	%r37, [%rd3];
	ld.shared.u32 	%r38, [%rd3+8];
	add.s32 	%r39, %r37, %r38;
	st.shared.u32 	[%rd3], %r39;
	bar.sync 	0;
	ld.shared.u32 	%r40, [%rd3];
	ld.shared.u32 	%r41, [%rd3+4];
	add.s32 	%r42, %r40, %r41;
	st.shared.u32 	[%rd3], %r42;

BB7_12:
	setp.ne.s32	%p8, %r1, 0;
	@%p8 bra 	BB7_14;

	cvta.to.global.u64 	%rd13, %rd6;
	ld.shared.u32 	%r43, [sumLandView$__cuda_local_var_36469_29_non_const_sdata];
	mul.wide.u32 	%rd14, %r10, 4;
	add.s64 	%rd15, %rd13, %rd14;
	st.global.u32 	[%rd15], %r43;

BB7_14:
	ret;
}

.visible .entry sumLandViewTan(
	.param .u64 sumLandViewTan_param_0,
	.param .u32 sumLandViewTan_param_1,
	.param .u64 sumLandViewTan_param_2,
	.param .u8 sumLandViewTan_param_3,
	.param .u64 sumLandViewTan_param_4
)
{
	.reg .pred 	%p<10>;
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<47>;
	.reg .s64 	%rd<17>;
	// demoted variable
	.shared .align 4 .b8 sumLandViewTan$__cuda_local_var_36503_29_non_const_sdata[2048];

	ld.param.u64 	%rd6, [sumLandViewTan_param_0];
	ld.param.u32 	%r10, [sumLandViewTan_param_1];
	ld.param.u64 	%rd7, [sumLandViewTan_param_2];
	ld.param.u8 	%rs1, [sumLandViewTan_param_3];
	ld.param.u64 	%rd5, [sumLandViewTan_param_4];
	cvta.to.global.u64 	%rd1, %rd7;
	cvta.to.global.u64 	%rd2, %rd6;
	mov.u32 	%r11, %ctaid.x;
	shl.b32 	%r12, %r11, 9;
	mov.u32 	%r1, %tid.x;
	add.s32 	%r45, %r12, %r1;
	mov.u32 	%r13, %nctaid.x;
	shl.b32 	%r3, %r13, 9;
	mul.wide.u32 	%rd8, %r1, 4;
	mov.u64 	%rd9, sumLandViewTan$__cuda_local_var_36503_29_non_const_sdata;
	add.s64 	%rd3, %rd9, %rd8;
	mov.u32 	%r46, 0;
	st.shared.u32 	[%rd3], %r46;
	setp.ge.u32	%p1, %r45, %r10;
	@%p1 bra 	BB8_5;

BB8_1:
	mul.wide.u32 	%rd10, %r45, 4;
	add.s64 	%rd11, %rd2, %rd10;
	ld.global.u32 	%r6, [%rd11];
	setp.lt.s32	%p2, %r6, 0;
	@%p2 bra 	BB8_4;

	cvt.s64.s32	%rd12, %r6;
	add.s64 	%rd13, %rd1, %rd12;
	ld.global.u8 	%rs2, [%rd13];
	setp.ne.s16	%p3, %rs2, %rs1;
	@%p3 bra 	BB8_4;

	add.s32 	%r46, %r46, 1;
	st.shared.u32 	[%rd3], %r46;

BB8_4:
	add.s32 	%r45, %r45, %r3;
	setp.lt.u32	%p4, %r45, %r10;
	@%p4 bra 	BB8_1;

BB8_5:
	bar.sync 	0;
	setp.gt.u32	%p5, %r1, 255;
	@%p5 bra 	BB8_7;

	ld.shared.u32 	%r16, [%rd3];
	ld.shared.u32 	%r17, [%rd3+1024];
	add.s32 	%r18, %r16, %r17;
	st.shared.u32 	[%rd3], %r18;

BB8_7:
	bar.sync 	0;
	setp.gt.u32	%p6, %r1, 127;
	@%p6 bra 	BB8_9;

	ld.shared.u32 	%r19, [%rd3];
	ld.shared.u32 	%r20, [%rd3+512];
	add.s32 	%r21, %r19, %r20;
	st.shared.u32 	[%rd3], %r21;

BB8_9:
	bar.sync 	0;
	setp.gt.u32	%p7, %r1, 63;
	@%p7 bra 	BB8_11;

	ld.shared.u32 	%r22, [%rd3];
	ld.shared.u32 	%r23, [%rd3+256];
	add.s32 	%r24, %r22, %r23;
	st.shared.u32 	[%rd3], %r24;

BB8_11:
	bar.sync 	0;
	setp.gt.u32	%p8, %r1, 31;
	@%p8 bra 	BB8_13;

	ld.shared.u32 	%r25, [%rd3];
	ld.shared.u32 	%r26, [%rd3+128];
	add.s32 	%r27, %r25, %r26;
	st.shared.u32 	[%rd3], %r27;
	bar.sync 	0;
	ld.shared.u32 	%r28, [%rd3];
	ld.shared.u32 	%r29, [%rd3+64];
	add.s32 	%r30, %r28, %r29;
	st.shared.u32 	[%rd3], %r30;
	bar.sync 	0;
	ld.shared.u32 	%r31, [%rd3];
	ld.shared.u32 	%r32, [%rd3+32];
	add.s32 	%r33, %r31, %r32;
	st.shared.u32 	[%rd3], %r33;
	bar.sync 	0;
	ld.shared.u32 	%r34, [%rd3];
	ld.shared.u32 	%r35, [%rd3+16];
	add.s32 	%r36, %r34, %r35;
	st.shared.u32 	[%rd3], %r36;
	bar.sync 	0;
	ld.shared.u32 	%r37, [%rd3];
	ld.shared.u32 	%r38, [%rd3+8];
	add.s32 	%r39, %r37, %r38;
	st.shared.u32 	[%rd3], %r39;
	bar.sync 	0;
	ld.shared.u32 	%r40, [%rd3];
	ld.shared.u32 	%r41, [%rd3+4];
	add.s32 	%r42, %r40, %r41;
	st.shared.u32 	[%rd3], %r42;

BB8_13:
	setp.ne.s32	%p9, %r1, 0;
	@%p9 bra 	BB8_15;

	cvta.to.global.u64 	%rd14, %rd5;
	ld.shared.u32 	%r43, [sumLandViewTan$__cuda_local_var_36503_29_non_const_sdata];
	mul.wide.u32 	%rd15, %r11, 4;
	add.s64 	%rd16, %rd14, %rd15;
	st.global.u32 	[%rd16], %r43;

BB8_15:
	ret;
}

.visible .entry clearView(
	.param .u64 clearView_param_0,
	.param .u32 clearView_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<9>;
	.reg .s64 	%rd<5>;


	ld.param.u64 	%rd1, [clearView_param_0];
	ld.param.u32 	%r2, [clearView_param_1];
	mov.u32 	%r3, %ctaid.y;
	mov.u32 	%r4, %nctaid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r6, %r3, %r4, %r5;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r8;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB9_2;

	cvta.to.global.u64 	%rd2, %rd1;
	cvt.s64.s32	%rd3, %r1;
	add.s64 	%rd4, %rd2, %rd3;
	mov.u16 	%rs1, 0;
	st.global.u8 	[%rd4], %rs1;

BB9_2:
	ret;
}

.visible .entry clearViewTan(
	.param .u64 clearViewTan_param_0,
	.param .u32 clearViewTan_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .s32 	%r<10>;
	.reg .s64 	%rd<5>;


	ld.param.u64 	%rd1, [clearViewTan_param_0];
	ld.param.u32 	%r2, [clearViewTan_param_1];
	mov.u32 	%r3, %ctaid.y;
	mov.u32 	%r4, %nctaid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r6, %r3, %r4, %r5;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r8;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB10_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	mov.u32 	%r9, -1;
	st.global.u32 	[%rd4], %r9;

BB10_2:
	ret;
}

.visible .entry addView(
	.param .u64 addView_param_0,
	.param .u64 addView_param_1,
	.param .u32 addView_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .s32 	%r<12>;
	.reg .s64 	%rd<9>;


	ld.param.u64 	%rd1, [addView_param_0];
	ld.param.u64 	%rd2, [addView_param_1];
	ld.param.u32 	%r2, [addView_param_2];
	mov.u32 	%r3, %ctaid.y;
	mov.u32 	%r4, %nctaid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r6, %r3, %r4, %r5;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r8;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB11_2;

	cvta.to.global.u64 	%rd3, %rd2;
	cvta.to.global.u64 	%rd4, %rd1;
	cvt.s64.s32	%rd5, %r1;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u8 	%r9, [%rd6];
	mul.wide.s32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.u32 	%r10, [%rd8];
	add.s32 	%r11, %r10, %r9;
	st.global.u32 	[%rd8], %r11;

BB11_2:
	ret;
}

.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
{
	.local .align 8 .b8 	__local_depot12[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<9>;
	.reg .s32 	%r<42>;
	.reg .s64 	%rd<99>;
	.reg .f64 	%fd<5>;


	mov.u64 	%SPL, __local_depot12;
	ld.param.f64 	%fd4, [__internal_trig_reduction_slowpathd_param_0];
	ld.param.u64 	%rd37, [__internal_trig_reduction_slowpathd_param_1];
	add.u64 	%rd38, %SPL, 0;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd4;
	}
	and.b32  	%r40, %r1, -2147483648;
	shr.u32 	%r3, %r1, 20;
	bfe.u32 	%r4, %r1, 20, 11;
	setp.eq.s32	%p1, %r4, 2047;
	@%p1 bra 	BB12_14;

	add.s32 	%r15, %r4, -1024;
	shr.u32 	%r16, %r15, 6;
	mov.u32 	%r17, 15;
	sub.s32 	%r5, %r17, %r16;
	mov.u32 	%r18, 19;
	sub.s32 	%r19, %r18, %r16;
	mov.u32 	%r20, 18;
	min.s32 	%r6, %r20, %r19;
	setp.lt.s32	%p2, %r5, %r6;
	mov.u64 	%rd92, %rd38;
	@%p2 bra 	BB12_3;

	mov.u64 	%rd93, 0;
	bra.uni 	BB12_5;

BB12_3:
	mov.b64 	 %rd41, %fd4;
	shl.b64 	%rd42, %rd41, 11;
	or.b64  	%rd3, %rd42, -9223372036854775808;
	bfe.u32 	%r21, %r1, 20, 11;
	add.s32 	%r22, %r21, -1024;
	shr.u32 	%r23, %r22, 6;
	sub.s32 	%r25, %r17, %r23;
	mul.wide.s32 	%rd43, %r25, 8;
	mov.u64 	%rd44, __cudart_i2opi_d;
	add.s64 	%rd90, %rd44, %rd43;
	mov.u64 	%rd93, 0;
	mov.u64 	%rd91, %rd38;
	mov.u64 	%rd89, %rd38;
	mov.u32 	%r39, %r5;

BB12_4:
	.pragma "nounroll";
	mov.u32 	%r7, %r39;
	mov.u64 	%rd6, %rd89;
	ld.const.u64 	%rd47, [%rd90];
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi, clo, chi;
	mov.b64         {alo,ahi}, %rd47;    
	mov.b64         {blo,bhi}, %rd3;    
	mov.b64         {clo,chi}, %rd93;    
	mad.lo.cc.u32   r0, alo, blo, clo;
	madc.hi.cc.u32  r1, alo, blo, chi;
	madc.hi.u32     r2, alo, bhi,   0;
	mad.lo.cc.u32   r1, alo, bhi,  r1;
	madc.hi.cc.u32  r2, ahi, blo,  r2;
	madc.hi.u32     r3, ahi, bhi,   0;
	mad.lo.cc.u32   r1, ahi, blo,  r1;
	madc.lo.cc.u32  r2, ahi, bhi,  r2;
	addc.u32        r3,  r3,   0;     
	mov.b64         %rd45, {r0,r1};      
	mov.b64         %rd46, {r2,r3};      
	}
	// inline asm
	st.local.u64 	[%rd91], %rd45;
	add.s32 	%r8, %r7, 1;
	sub.s32 	%r26, %r8, %r5;
	mul.wide.s32 	%rd50, %r26, 8;
	add.s64 	%rd91, %rd38, %rd50;
	add.s64 	%rd90, %rd90, 8;
	add.s64 	%rd92, %rd6, 8;
	setp.lt.s32	%p3, %r8, %r6;
	mov.u64 	%rd13, %rd92;
	mov.u64 	%rd93, %rd46;
	mov.u64 	%rd89, %rd13;
	mov.u32 	%r39, %r8;
	@%p3 bra 	BB12_4;

BB12_5:
	st.local.u64 	[%rd92], %rd93;
	ld.local.u64 	%rd94, [%rd38+16];
	ld.local.u64 	%rd95, [%rd38+24];
	and.b32  	%r9, %r3, 63;
	setp.eq.s32	%p4, %r9, 0;
	@%p4 bra 	BB12_7;

	mov.u32 	%r27, 64;
	sub.s32 	%r28, %r27, %r9;
	shl.b64 	%rd51, %rd95, %r9;
	shr.u64 	%rd52, %rd94, %r28;
	or.b64  	%rd95, %rd51, %rd52;
	shl.b64 	%rd53, %rd94, %r9;
	ld.local.u64 	%rd54, [%rd38+8];
	shr.u64 	%rd55, %rd54, %r28;
	or.b64  	%rd94, %rd55, %rd53;

BB12_7:
	cvta.to.local.u64 	%rd56, %rd37;
	shr.u64 	%rd57, %rd95, 62;
	cvt.u32.u64	%r29, %rd57;
	shr.u64 	%rd58, %rd94, 62;
	shl.b64 	%rd59, %rd95, 2;
	or.b64  	%rd97, %rd59, %rd58;
	shl.b64 	%rd96, %rd94, 2;
	shr.u64 	%rd60, %rd95, 61;
	cvt.u32.u64	%r30, %rd60;
	and.b32  	%r31, %r30, 1;
	add.s32 	%r32, %r31, %r29;
	neg.s32 	%r33, %r32;
	setp.eq.s32	%p5, %r40, 0;
	selp.b32	%r34, %r32, %r33, %p5;
	st.local.u32 	[%rd56], %r34;
	setp.eq.s32	%p6, %r31, 0;
	@%p6 bra 	BB12_9;

	mov.u64 	%rd64, 0;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd64;
	mov.b64         {a2,a3}, %rd64;
	mov.b64         {b0,b1}, %rd96;
	mov.b64         {b2,b3}, %rd97;
	sub.cc.u32      r0, a0, b0; 
	subc.cc.u32     r1, a1, b1; 
	subc.cc.u32     r2, a2, b2; 
	subc.u32        r3, a3, b3; 
	mov.b64         %rd61, {r0,r1};
	mov.b64         %rd62, {r2,r3};
	}
	// inline asm
	xor.b32  	%r40, %r40, -2147483648;
	mov.u64 	%rd97, %rd62;
	mov.u64 	%rd96, %rd61;

BB12_9:
	clz.b64 	%r41, %rd97;
	setp.eq.s32	%p7, %r41, 0;
	@%p7 bra 	BB12_11;

	shl.b64 	%rd67, %rd97, %r41;
	mov.u32 	%r35, 64;
	sub.s32 	%r36, %r35, %r41;
	shr.u64 	%rd68, %rd96, %r36;
	or.b64  	%rd97, %rd68, %rd67;

BB12_11:
	mov.u64 	%rd72, -3958705157555305931;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi;
	mov.b64         {alo,ahi}, %rd97;   
	mov.b64         {blo,bhi}, %rd72;   
	mul.lo.u32      r0, alo, blo;    
	mul.hi.u32      r1, alo, blo;    
	mad.lo.cc.u32   r1, alo, bhi, r1;
	madc.hi.u32     r2, alo, bhi,  0;
	mad.lo.cc.u32   r1, ahi, blo, r1;
	madc.hi.cc.u32  r2, ahi, blo, r2;
	madc.hi.u32     r3, ahi, bhi,  0;
	mad.lo.cc.u32   r2, ahi, bhi, r2;
	addc.u32        r3, r3,  0;      
	mov.b64         %rd69, {r0,r1};     
	mov.b64         %rd70, {r2,r3};     
	}
	// inline asm
	setp.lt.s64	%p8, %rd70, 1;
	mov.u64 	%rd98, %rd70;
	@%p8 bra 	BB12_13;

	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd69;
	mov.b64         {a2,a3}, %rd70;
	mov.b64         {b0,b1}, %rd69;
	mov.b64         {b2,b3}, %rd70;
	add.cc.u32      r0, a0, b0; 
	addc.cc.u32     r1, a1, b1; 
	addc.cc.u32     r2, a2, b2; 
	addc.u32        r3, a3, b3; 
	mov.b64         %rd73, {r0,r1};
	mov.b64         %rd74, {r2,r3};
	}
	// inline asm
	add.s32 	%r41, %r41, 1;
	mov.u64 	%rd98, %rd74;

BB12_13:
	cvt.u64.u32	%rd79, %r40;
	shl.b64 	%rd80, %rd79, 32;
	mov.u32 	%r37, 1022;
	sub.s32 	%r38, %r37, %r41;
	cvt.u64.u32	%rd81, %r38;
	shl.b64 	%rd82, %rd81, 52;
	add.s64 	%rd83, %rd98, 1;
	shr.u64 	%rd84, %rd83, 10;
	add.s64 	%rd85, %rd84, 1;
	shr.u64 	%rd86, %rd85, 1;
	add.s64 	%rd87, %rd86, %rd82;
	or.b64  	%rd88, %rd87, %rd80;
	mov.b64 	 %fd4, %rd88;

BB12_14:
	st.param.f64	[func_retval0+0], %fd4;
	ret;
}


